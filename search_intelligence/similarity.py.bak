"""
Document Similarity and Clustering Module

Provides document similarity analysis and clustering capabilities
using Legal BERT embeddings and DBSCAN clustering.
"""

from collections import defaultdict
from datetime import datetime

import numpy as np
from loguru import logger
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity

from shared.simple_db import SimpleDB
from utilities.embeddings import get_embedding_service
from utilities.vector_store import get_vector_store

# Logger is now imported globally from loguru


class DocumentSimilarityAnalyzer:
    """Analyze document similarity using embeddings."""

    def __init__(self, collection: str = "emails"):
        """Initialize similarity analyzer."""
        self.logger = logger
        self.collection = collection
        self.embedding_service = get_embedding_service()
        self.vector_store = get_vector_store(collection)
        self.db = SimpleDB()

    def find_similar_documents(
        self, doc_id: str, limit: int = 10, threshold: float = 0.7
    ) -> list[dict]:
        """
        Find documents similar to a given document.

        Args:
            doc_id: Source document ID
            limit: Maximum similar documents to return
            threshold: Minimum similarity threshold (0-1)

        Returns:
            List of similar documents with scores
        """
        try:
            # Get document vector
            doc_vector = self._get_document_vector(doc_id)
            if doc_vector is None:
                return []

            # Search for similar vectors
            results = self.vector_store.search(
                vector=doc_vector, limit=limit + 1, score_threshold=threshold  # +1 to exclude self
            )

            # Filter and format results
            similar_docs = []
            for result in results:
                if result["id"] != doc_id:
                    similar_docs.append(
                        {
                            "id": result["id"],
                            "similarity_score": result["score"],
                            "metadata": result.get("payload", {}),
                        }
                    )

            return similar_docs

        except Exception as e:
            logger.error(f"Similarity search failed for {doc_id}: {e}")
            return []

    def compute_pairwise_similarity(self, doc_ids: list[str]) -> np.ndarray:
        """
        Compute pairwise similarity matrix for documents.

        Args:
            doc_ids: List of document IDs

        Returns:
            Similarity matrix (numpy array)
        """
        vectors = []
        valid_ids = []

        for doc_id in doc_ids:
            vector = self._get_document_vector(doc_id)
            if vector is not None:
                vectors.append(vector)
                valid_ids.append(doc_id)

        if not vectors:
            return np.array([])

        # Stack vectors and compute cosine similarity
        vector_matrix = np.vstack(vectors)
        similarity_matrix = cosine_similarity(vector_matrix)

        return similarity_matrix, valid_ids

    def _get_document_vector(self, doc_id: str) -> np.ndarray | None:
        """Get or generate document vector."""
        try:
            # Try to get from vector store
            result = self.vector_store.get(doc_id)
            if result and "vector" in result:
                return np.array(result["vector"])

            # Generate new vector
            doc = self._get_document_content(doc_id)
            if not doc:
                return None

            text = self._extract_text(doc)
            if not text:
                return None

            vector = self.embedding_service.encode(text)

            # Store for future use
            self.vector_store.upsert(
                id=doc_id,
                vector=vector.tolist(),
                payload={"content_type": doc.get("content_type", "unknown")},
            )

            return vector

        except Exception as e:
            logger.error(f"Failed to get vector for {doc_id}: {e}")
            return None

    def _get_document_content(self, doc_id: str) -> dict | None:
        """Get document content from database."""
        # Try content table first
        result = self.db.fetch_one("SELECT * FROM content_unified WHERE id = ?", (doc_id,))
        if result:
            return result

        # Try emails
        result = self.db.fetch_one(
            "SELECT * FROM emails WHERE message_id = ? OR id = ?", (doc_id, doc_id)
        )
        if result:
            result["source_type"] = "email"
            return result

        # Try documents
        result = self.db.fetch_one("SELECT * FROM documents WHERE chunk_id = ?", (doc_id,))
        if result:
            result["source_type"] = "document"

        return result

    def _extract_text(self, doc: dict) -> str:
        """Extract text from document."""
        return (
            doc.get("body", "")
            or doc.get("text_content", "")
            or doc.get("content_unified", "")
            or doc.get("subject", "")
        )


class DocumentClusterer:
    """Cluster documents using DBSCAN with similarity threshold."""

    def __init__(self, similarity_analyzer: DocumentSimilarityAnalyzer = None):
        """Initialize clusterer."""
        self.logger = logger
        self.analyzer = similarity_analyzer or DocumentSimilarityAnalyzer()
        self.db = SimpleDB()

    def cluster_documents(
        self, doc_ids: list[str], threshold: float = 0.7, min_samples: int = 2
    ) -> dict[int, list[str]]:
        """
        Cluster documents using DBSCAN.

        Args:
            doc_ids: List of document IDs to cluster
            threshold: Similarity threshold (0-1)
            min_samples: Minimum samples for core point

        Returns:
            Dict mapping cluster ID to document IDs
        """
        if len(doc_ids) < 2:
            return {0: doc_ids}

        try:
            # Compute similarity matrix
            similarity_matrix, valid_ids = self.analyzer.compute_pairwise_similarity(doc_ids)

            if len(valid_ids) < 2:
                return {0: valid_ids}

            # Convert similarity to distance for DBSCAN
            # Distance = 1 - similarity
            distance_matrix = 1 - similarity_matrix

            # Apply DBSCAN clustering
            # eps is the distance threshold (1 - similarity_threshold)
            clusterer = DBSCAN(eps=(1 - threshold), min_samples=min_samples, metric="precomputed")

            cluster_labels = clusterer.fit_predict(distance_matrix)

            # Group documents by cluster
            clusters = defaultdict(list)
            for idx, label in enumerate(cluster_labels):
                clusters[label].append(valid_ids[idx])

            # Log clustering results
            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
            n_noise = list(cluster_labels).count(-1)

            logger.info(
                f"Clustered {len(valid_ids)} documents into {n_clusters} clusters "
                f"({n_noise} noise points)"
            )

            return dict(clusters)

        except Exception as e:
            logger.error(f"Clustering failed: {e}")
            # Return all documents in single cluster on error
            return {0: doc_ids}

    def find_content_clusters(
        self, content_type: str = None, limit: int = 100, threshold: float = 0.7
    ) -> list[dict]:
        """
        Find clusters in content of specific type.

        Args:
            content_type: Type of content (email, document, etc)
            limit: Maximum documents to cluster
            threshold: Similarity threshold

        Returns:
            List of cluster information
        """
        try:
            # Get document IDs
            if content_type:
                query = "SELECT id FROM content_unified WHERE source_type = ? LIMIT ?"
                params = (content_type, limit)
            else:
                query = "SELECT id FROM content_unified LIMIT ?"
                params = (limit,)

            results = self.db.fetch(query, params)
            doc_ids = [r["content_id"] for r in results]

            if not doc_ids:
                return []

            # Perform clustering
            clusters = self.cluster_documents(doc_ids, threshold)

            # Format cluster information
            cluster_info = []
            for cluster_id, member_ids in clusters.items():
                if cluster_id != -1:  # Exclude noise points
                    cluster_info.append(
                        {
                            "cluster_id": cluster_id,
                            "size": len(member_ids),
                            "members": member_ids[:10],  # Sample of members
                            "total_members": len(member_ids),
                        }
                    )

            # Sort by cluster size
            cluster_info.sort(key=lambda x: x["size"], reverse=True)

            return cluster_info

        except Exception as e:
            logger.error(f"Content clustering failed: {e}")
            return []

    def store_cluster_relationships(self, clusters: dict[int, list[str]], ttl_seconds: int = 86400):
        """
        Store cluster relationships in database.

        Args:
            clusters: Dict mapping cluster ID to document IDs
            ttl_seconds: Cache TTL in seconds
        """
        try:
            for cluster_id, member_ids in clusters.items():
                if cluster_id == -1:  # Skip noise points
                    continue

                # Store relationships between cluster members
                for i, doc_id1 in enumerate(member_ids):
                    for doc_id2 in member_ids[i + 1 :]:
                        self.db.add_relationship_cache(
                            source_id=doc_id1,
                            target_id=doc_id2,
                            relationship_type="in_cluster_with",
                            relationship_data={
                                "cluster_id": cluster_id,
                                "clustered_at": datetime.now().isoformat(),
                            },
                            ttl_seconds=ttl_seconds,
                        )

        except Exception as e:
            logger.error(f"Failed to store cluster relationships: {e}")


def cluster_similar_content(
    threshold: float = 0.7,
    content_type: str = None,
    limit: int = 100,
    min_samples: int = 2,
    store_relationships: bool = True,
) -> list[dict]:
    """
    Convenience function to cluster similar content.

    Args:
        threshold: Similarity threshold (0-1)
        content_type: Optional content type filter
        limit: Maximum documents to cluster
        min_samples: Minimum samples for DBSCAN
        store_relationships: Whether to store in database

    Returns:
        List of cluster information
    """
    analyzer = DocumentSimilarityAnalyzer()
    clusterer = DocumentClusterer(analyzer)

    # Find clusters
    clusters = clusterer.find_content_clusters(
        source_type =content_type, limit=limit, threshold=threshold
    )

    # Store relationships if requested
    if store_relationships and clusters:
        # Convert list format back to dict for storage
        cluster_dict = {}
        for cluster_info in clusters:
            cluster_dict[cluster_info["cluster_id"]] = cluster_info["members"]
        clusterer.store_cluster_relationships(cluster_dict)

    return clusters
