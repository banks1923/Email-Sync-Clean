"""Topic Clustering and Entity Co-occurrence Analysis for Knowledge Graph.

Implements hierarchical clustering on Legal BERT embeddings and entity
co-occurrence analysis. Follows CLAUDE.md principles: simple patterns,
functions under 30 lines.
"""

import json
from collections import Counter, defaultdict

import numpy as np
from loguru import logger
from scipy.cluster.hierarchy import fcluster, linkage
from scipy.spatial.distance import pdist

from shared.simple_db import SimpleDB
from utilities.embeddings import get_embedding_service
from entity.main import EntityService

from .main import KnowledgeGraphService
from .similarity_analyzer import SimilarityAnalyzer

# Logger is now imported globally from loguru


class TopicClusteringService:
    """
    Discover topics and entity relationships through clustering and co-
    occurrence.
    """

    def __init__(self, db_path: str = "emails.db"):
        self.db = SimpleDB(db_path)
        self.kg_service = KnowledgeGraphService(db_path)
        self.similarity_analyzer = SimilarityAnalyzer(db_path)
        self.entity_service = EntityService()
        self.embedding_service = get_embedding_service()

    def perform_hierarchical_clustering(
        self, content_ids: list[str], distance_threshold: float = 0.5, method: str = "ward"
    ) -> dict:
        """
        Perform hierarchical clustering on content embeddings.
        """
        logger.info(f"Performing hierarchical clustering on {len(content_ids)} documents")

        # Get embeddings for all content
        embeddings = self._get_content_embeddings(content_ids)

        if len(embeddings) < 2:
            return {"error": "Need at least 2 documents for clustering"}

        # Convert to numpy array
        embedding_matrix = np.array(list(embeddings.values()))
        content_id_list = list(embeddings.keys())

        # Calculate distance matrix and perform clustering
        distances = pdist(embedding_matrix, metric="cosine")
        linkage_matrix = linkage(distances, method=method)

        # Get cluster assignments
        clusters = fcluster(linkage_matrix, distance_threshold, criterion="distance")

        # Organize results by cluster
        cluster_results = self._organize_clusters(content_id_list, clusters)

        # Store cluster relationships in knowledge graph
        self._store_cluster_relationships(cluster_results)

        return {
            "num_clusters": len(set(clusters)),
            "clusters": cluster_results,
            "method": method,
            "distance_threshold": distance_threshold,
        }

    def _get_content_embeddings(self, content_ids: list[str]) -> dict[str, np.ndarray]:
        """
        Get embeddings for content items.
        """
        embeddings = {}

        for content_id in content_ids:
            # Get content and generate embedding
            content = self.db.fetch_one(
                "SELECT title, content FROM content_unified WHERE id = ?", (content_id,)
            )

            if content:
                text = f"{content['title'] or ''} {content['content_unified'] or ''}"[:5000]
                embedding = self.embedding_service.encode(text)
                embeddings[content_id] = embedding

        return embeddings

    def _organize_clusters(
        self, content_ids: list[str], cluster_assignments: np.ndarray
    ) -> dict[int, dict]:
        """
        Organize clustering results by cluster ID.
        """
        clusters = defaultdict(list)

        for content_id, cluster_id in zip(content_ids, cluster_assignments):
            clusters[int(cluster_id)].append(content_id)

        # Convert to structured format with labels
        result = {}
        for cluster_id, members in clusters.items():
            result[cluster_id] = {
                "members": members,
                "size": len(members),
                "label": self._generate_cluster_label(members),
            }

        return result

    def _generate_cluster_label(self, content_ids: list[str]) -> str:
        """
        Generate a descriptive label for a cluster based on entities/keywords.
        """
        # Extract all entities from cluster members
        all_entities = []

        for content_id in content_ids[:10]:  # Sample first 10 for efficiency
            content = self.db.fetch_one(
                "SELECT content FROM content_unified WHERE id = ?", (content_id,)
            )

            if content and content["content_unified"]:
                # Extract entities
                result = self.entity_service.extract_email_entities(
                    content_id, content["content_unified"][:1000], {}  # First 1000 chars
                )

                if result.get("success") and result.get("entities"):
                    for entity in result["entities"]:
                        all_entities.append(entity.get("text", ""))

        # Find most common entities
        if all_entities:
            entity_counts = Counter(all_entities)
            top_entities = entity_counts.most_common(3)
            return ", ".join([entity for entity, _ in top_entities])

        return f"Cluster {len(content_ids)} documents"

    def _store_cluster_relationships(self, clusters: dict[int, dict]) -> None:
        """
        Store cluster memberships as edges in knowledge graph.
        """
        for cluster_id, cluster_info in clusters.items():
            members = cluster_info["members"]

            # Create edges between cluster members
            for i, member1 in enumerate(members):
                for member2 in members[i + 1 :]:
                    self.kg_service.add_edge(
                        member1,
                        member2,
                        "same_cluster",
                        strength=0.7,
                        metadata={
                            "cluster_id": cluster_id,
                            "cluster_label": cluster_info["label"],
                            "cluster_size": cluster_info["size"],
                        },
                    )

    def calculate_entity_cooccurrence(
        self, content_type: str = None, min_cooccurrence: int = 2
    ) -> dict:
        """
        Calculate entity co-occurrence matrix across content.
        """
        logger.info("Calculating entity co-occurrence matrix")

        # Get content to analyze
        query = "SELECT id, content FROM content_unified"
        params = []
        if content_type:
            query += " WHERE source_type = ?"
            params.append(content_type)

        content_items = self.db.fetch(query, tuple(params))

        # Extract entities and build co-occurrence matrix
        cooccurrence_matrix = defaultdict(lambda: defaultdict(int))
        entity_documents = defaultdict(set)  # Track which docs contain each entity

        for item in content_items:
            entities = self._extract_entities_from_content(item["content_id"], item["content_unified"])

            # Update co-occurrence counts
            for i, entity1 in enumerate(entities):
                entity_documents[entity1].add(item["content_id"])
                for entity2 in entities[i + 1 :]:
                    cooccurrence_matrix[entity1][entity2] += 1
                    cooccurrence_matrix[entity2][entity1] += 1

        # Filter by minimum co-occurrence threshold
        filtered_matrix = self._filter_cooccurrence_matrix(cooccurrence_matrix, min_cooccurrence)

        # Store significant co-occurrences as graph edges
        self._store_cooccurrence_relationships(filtered_matrix, entity_documents)

        return {
            "total_entities": len(entity_documents),
            "significant_pairs": len(filtered_matrix),
            "min_cooccurrence": min_cooccurrence,
            "top_cooccurrences": self._get_top_cooccurrences(filtered_matrix, 10),
        }

    def _extract_entities_from_content(self, content_id: str, content: str) -> list[str]:
        """
        Extract unique entities FROM content_unified.
        """
        if not content:
            return []

        # Use entity service to extract entities
        result = self.entity_service.extract_email_entities(content_id, content[:5000], {})

        if result.get("success") and result.get("entities"):
            # Return unique entity texts
            entities = set()
            for entity in result["entities"]:
                text = entity.get("text", "").strip()
                if text and len(text) > 1:  # Filter out single chars
                    entities.add(text)
            return list(entities)

        return []

    def _filter_cooccurrence_matrix(
        self, matrix: dict[str, dict[str, int]], min_count: int
    ) -> list[tuple[str, str, int]]:
        """
        Filter co-occurrence matrix by minimum count.
        """
        filtered = []
        seen_pairs = set()

        for entity1, cooccurrences in matrix.items():
            for entity2, count in cooccurrences.items():
                if count >= min_count:
                    # Avoid duplicates (A,B) and (B,A)
                    pair = tuple(sorted([entity1, entity2]))
                    if pair not in seen_pairs:
                        seen_pairs.add(pair)
                        filtered.append((entity1, entity2, count))

        return filtered

    def _store_cooccurrence_relationships(
        self, cooccurrences: list[tuple[str, str, int]], entity_documents: dict[str, set[str]]
    ) -> None:
        """
        Store entity co-occurrences as knowledge graph edges.
        """
        for entity1, entity2, count in cooccurrences:
            # Get documents containing both entities
            docs1 = entity_documents.get(entity1, set())
            docs2 = entity_documents.get(entity2, set())
            shared_docs = docs1.intersection(docs2)

            # Calculate strength based on co-occurrence frequency
            strength = min(1.0, count / 10.0)  # Normalize to 0-1

            # Store as entity co-occurrence edge
            # Note: We're storing entity pairs, not content relationships
            # This could be enhanced to link to actual content items
            for doc_id in list(shared_docs)[:5]:  # Limit to 5 docs per pair
                self.kg_service.add_edge(
                    doc_id,
                    doc_id,  # Self-edge with metadata
                    "contains_entities",
                    strength=strength,
                    metadata={
                        "entity_pair": [entity1, entity2],
                        "cooccurrence_count": count,
                        "total_shared_docs": len(shared_docs),
                    },
                )

    def _get_top_cooccurrences(
        self, cooccurrences: list[tuple[str, str, int]], limit: int
    ) -> list[dict]:
        """
        Get top co-occurring entity pairs.
        """
        # Sort by count
        sorted_pairs = sorted(cooccurrences, key=lambda x: x[2], reverse=True)

        result = []
        for entity1, entity2, count in sorted_pairs[:limit]:
            result.append({"entity1": entity1, "entity2": entity2, "count": count})

        return result

    def update_clusters_incrementally(
        self, new_content_ids: list[str], recalculate_threshold: int = 50
    ) -> dict:
        """
        Update clusters with new content without full recalculation.
        """
        logger.info(f"Incrementally updating clusters with {len(new_content_ids)} new items")

        # If too many new items, trigger full recalculation
        if len(new_content_ids) > recalculate_threshold:
            all_content = self.db.fetch("SELECT id FROM content_unified")
            all_ids = [item["content_id"] for item in all_content]
            return self.perform_hierarchical_clustering(all_ids)

        # Otherwise, find best cluster for each new item
        existing_clusters = self._get_existing_clusters()

        assignments = {}
        for content_id in new_content_ids:
            best_cluster = self._find_best_cluster(content_id, existing_clusters)
            assignments[content_id] = best_cluster

            # Add to cluster in graph
            if best_cluster:
                self._add_to_cluster(content_id, best_cluster)

        return {
            "incremental_update": True,
            "new_assignments": assignments,
            "items_processed": len(new_content_ids),
        }

    def _get_existing_clusters(self) -> dict[int, list[str]]:
        """
        Retrieve existing cluster assignments from graph.
        """
        # Query for same_cluster edges
        cluster_edges = self.db.fetch(
            """
            SELECT edge_metadata
            FROM kg_edges
            WHERE relationship_type = 'same_cluster'
            """
        )

        clusters = defaultdict(set)
        for edge in cluster_edges:
            if edge["edge_metadata"]:
                try:
                    metadata = json.loads(edge["edge_metadata"])
                    cluster_id = metadata.get("cluster_id")
                    if cluster_id:
                        # Would need to track members differently
                        # This is simplified
                        pass
                except json.JSONDecodeError:
                    continue

        return dict(clusters)

    def _find_best_cluster(
        self, content_id: str, existing_clusters: dict[int, list[str]]
    ) -> int | None:
        """
        Find the best matching cluster for new content.
        """
        if not existing_clusters:
            return None

        # Get embedding for new content
        embeddings = self._get_content_embeddings([content_id])
        if content_id not in embeddings:
            return None

        new_embedding = embeddings[content_id]

        # Calculate average similarity to each cluster
        best_cluster = None
        best_similarity = 0.0

        for cluster_id, members in existing_clusters.items():
            # Sample members for efficiency
            sample = members[:10] if len(members) > 10 else members
            member_embeddings = self._get_content_embeddings(sample)

            if member_embeddings:
                # Calculate average similarity
                similarities = []
                for member_id, member_emb in member_embeddings.items():
                    sim = self.similarity_analyzer._cosine_similarity(new_embedding, member_emb)
                    similarities.append(sim)

                avg_similarity = np.mean(similarities)
                if avg_similarity > best_similarity:
                    best_similarity = avg_similarity
                    best_cluster = cluster_id

        # Only assign if similarity is above threshold
        if best_similarity > 0.6:
            return best_cluster

        return None

    def _add_to_cluster(self, content_id: str, cluster_id: int) -> None:
        """
        Add content to an existing cluster.
        """
        # Get a sample member from the cluster to create edge
        cluster_edges = self.db.fetch_one(
            """
            SELECT source_node_id, target_node_id, edge_metadata
            FROM kg_edges
            WHERE relationship_type = 'same_cluster'
            AND edge_metadata LIKE ?
            LIMIT 1
            """,
            (f'%"cluster_id": {cluster_id}%',),
        )

        if cluster_edges:
            # Create edge to existing cluster member
            self.kg_service.add_edge(
                content_id,
                cluster_edges["source_node_id"],
                "same_cluster",
                strength=0.7,
                metadata={"cluster_id": cluster_id, "incremental_addition": True},
            )

    def get_topic_statistics(self) -> dict:
        """
        Get statistics about topics and entity co-occurrences.
        """
        stats = {}

        # Cluster statistics
        cluster_stats = self.db.fetch_one(
            """
            SELECT
                COUNT(DISTINCT edge_metadata) as num_clusters,
                COUNT(*) as cluster_edges
            FROM kg_edges
            WHERE relationship_type = 'same_cluster'
            """
        )
        stats["clusters"] = cluster_stats or {}

        # Entity co-occurrence statistics
        cooccurrence_stats = self.db.fetch_one(
            """
            SELECT COUNT(*) as cooccurrence_edges
            FROM kg_edges
            WHERE relationship_type = 'contains_entities'
            """
        )
        stats["entity_cooccurrences"] = cooccurrence_stats or {}

        # Top entities by frequency
        # This would require entity tracking table
        stats["top_entities"] = []

        return stats


def get_topic_clustering_service(db_path: str = "emails.db") -> TopicClusteringService:
    """
    Factory function to get topic clustering service instance.
    """
    return TopicClusteringService(db_path)
