#!/usr/bin/env python3
"""
AI-Powered Database Search CLI - Clean Version
Uses new clean services architecture with database-only operation
Usage: scripts/vsearch [command] [options]
"""

import argparse
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Direct imports following CLAUDE.md "Direct > Indirect" principle
# Lazy import for search functions to avoid model loading timeout
from shared.error_handler import ErrorHandler
from shared.health_check import run_health_check
from shared.simple_db import SimpleDB
from tools.cli.quarantine_handler import add_quarantine_commands, handle_quarantine_command

# Global cache for expensive imports
_search_functions = None


def _get_search_functions():
    """Lazy load search functions to avoid timeout on CLI startup."""
    global _search_functions
    if _search_functions is None:
        from search_intelligence.basic_search import search, semantic_search, vector_store_available
        _search_functions = {
            'search': search,
            'semantic_search': semantic_search,
            'vector_store_available': vector_store_available
        }
    return _search_functions


def warmup_command():
    """Preload models and services for faster subsequent commands."""
    print("⏳ Warming up models and services...")
    
    try:
        import time
        start_time = time.time()
        
        # Load search functions (which loads Legal BERT model)
        print("  Loading search functions...")
        search_funcs = _get_search_functions()
        
        # Test basic functionality
        print("  Testing vector store connection...")
        if search_funcs['vector_store_available']():
            print("  ✅ Vector store ready")
        else:
            print("  ⚠️  Vector store not available")
        
        elapsed = time.time() - start_time
        print(f"✅ Warmup completed in {elapsed:.1f}s")
        print("   Subsequent commands will run faster!")
        
    except Exception as e:
        print(f"❌ Warmup failed: {e}")
        return False
    
    return True


def search_command(query, limit=5, verbose=False, filters=None, search_mode="hybrid", use_chunk_aggregation=None):
    """Simplified search using direct search coordination module"""
    # Display search info with filters
    filter_info = _format_filter_info(filters)
    
    print(f"🤖 AI-Powered Search for: '{query}'{filter_info}")
    
    try:
        # Lazy load search functions
        search_funcs = _get_search_functions()
        
        # Direct search using coordination module - Simple > Complex
        if search_mode == "semantic":
            print("🧠 Running semantic search...")
            results = search_funcs['semantic_search'](query, limit=limit, filters=filters, use_chunk_aggregation=use_chunk_aggregation)
            search_type = "🧠 Semantic Search"
        elif search_mode == "keyword":
            print("🔤 Running keyword search...")
            db = SimpleDB()
            results = db.search_content(query, limit=limit, filters=filters)
            search_type = "🔤 Keyword Search"
        else:  # hybrid
            print("🔍 Running hybrid search...")
            results = search_funcs['search'](query, limit=limit, filters=filters, use_chunk_aggregation=use_chunk_aggregation)
            search_type = "🔍 Hybrid Search"
        
        if results:
            print(f"✅ Found {len(results)} matches")
            display_search_results(results, search_type, search_mode)
            return True
        else:
            print("❌ No results found")
            print("💡 Try different keywords or check if content has been indexed")
            return False
            
    except Exception as e:
        if verbose:
            import traceback
            print(f"❌ Search error: {e}")
            print(traceback.format_exc())
        else:
            user_msg = ErrorHandler.format_user_message(
                {
                    "error": str(e),
                    "details": {"category": ErrorHandler.DATABASE_ERROR, "context": "searching"},
                }
            )
            print(f"❌ {user_msg}")

            suggestion = ErrorHandler.get_recovery_suggestion(
                {"error": str(e), "details": {"category": ErrorHandler.DATABASE_ERROR}}
            )
            if suggestion:
                print(f"💡 {suggestion}")
        return False


def _format_filter_info(filters):
    """Format filter information for display"""
    if not filters:
        return ""
    
    filter_parts = []
    if filters.get("since"):
        filter_parts.append(f"since {filters['since']}")
    if filters.get("until"):
        filter_parts.append(f"until {filters['until']}")
    if filters.get("content_types"):
        types_str = ",".join(filters["content_types"])
        filter_parts.append(f"types: {types_str}")
    if filters.get("tags"):
        tags_str = (
            ",".join(filters["tags"]) if isinstance(filters["tags"], list) else filters["tags"]
        )
        logic = filters.get("tag_logic", "OR")
        filter_parts.append(f"tags: {tags_str} ({logic})")
    
    return f" (Filters: {', '.join(filter_parts)})" if filter_parts else ""


def _format_score_info(result, search_mode):
    """Format search score information based on search mode"""
    if search_mode == "hybrid":
        # Show hybrid search ranking information
        rrf_score = result.get("rrf_score")
        keyword_rank = result.get("keyword_rank")
        semantic_rank = result.get("semantic_rank")
        
        if rrf_score is not None:
            score_parts = [f"RRF: {rrf_score:.3f}"]
            if keyword_rank:
                score_parts.append(f"KW: #{keyword_rank}")
            if semantic_rank:
                score_parts.append(f"SEM: #{semantic_rank}")
            return f" ({', '.join(score_parts)})"
    elif search_mode == "semantic":
        semantic_score = result.get("semantic_score", result.get("score"))
        if semantic_score is not None:
            return f" (Score: {semantic_score:.3f})"
    
    return ""


def _get_content_preview(result):
    """Extract content preview from result"""
    # Try different content fields
    for field in ["highlighted_snippet", "snippet", "content", "body"]:
        content = result.get(field)
        if content:
            if field == "highlighted_snippet":
                return content  # Already formatted
            else:
                # Clean and truncate - increased to 500 chars for better previews
                preview = str(content)[:500].replace("\n", " ").strip()
                return f"{preview}..." if len(str(content)) > 500 else preview
    
    return "No content preview available"


def display_search_results(results, search_type, search_mode="hybrid"):
    """Unified display function for all search result types"""
    try:
        print(f"\n=== {search_type} Results ===")
        
        # Show vector store status for hybrid/semantic searches  
        if search_mode in ["hybrid", "semantic"]:
            try:
                search_funcs = _get_search_functions()
                vector_available = search_funcs['vector_store_available']()
                print(f"Vector Service: {'✅ Connected' if vector_available else '❌ Unavailable (keyword fallback)'}")
            except:
                print("Vector Service: ❓ Status unknown")

        for i, result in enumerate(results, 1):
            # Get content type and icon
            content_type = result.get("content_type", result.get("type", "unknown"))
            type_icons = {"email": "📧", "pdf": "📄", "transcript": "🎙️", "document": "📄", "note": "📝"}
            icon = type_icons.get(content_type, "📄")

            # Format score information
            score_info = _format_score_info(result, search_mode)
            
            print(f"\n--- {icon} Result {i}{score_info} ---")
            print(f"Title: {result.get('title', 'No title')}")

            # Show metadata based on content type
            if content_type == "email":
                if result.get("sender"):
                    print(f"From: {result['sender']}")
                if result.get("date") or result.get("datetime_utc"):
                    date_str = result.get("date", result.get("datetime_utc", ""))
                    print(f"Date: {date_str}")

            # Show content preview
            content_preview = _get_content_preview(result)
            if content_preview:
                print(f"Content: {content_preview}")
                
    except Exception as e:
        print(f"❌ Database operation failed while searching")
        if '--verbose' in sys.argv:
            import traceback
            print(f"Debug: {e}")
            traceback.print_exc()




def health_command(verbose=False):
    """Run comprehensive health check on all services"""
    print("🏥 Running System Health Check...")

    try:
        health_status = run_health_check()

        # Overall status
        if health_status["overall_health"]:
            print(f"✅ System Status: HEALTHY ({health_status['summary']})")
        else:
            print(f"⚠️  System Status: DEGRADED ({health_status['summary']})")

        # Individual service status
        for service_name, status in health_status["services"].items():
            if status["healthy"]:
                icon = "✅"
            else:
                icon = "❌" if service_name != "qdrant" else "⚠️"

            print(f"\n{icon} {service_name.title()} Service:")

            if verbose or not status["healthy"]:
                for key, value in status.items():
                    if key != "healthy" and key != "service":
                        print(f"  {key}: {value}")
            elif status["healthy"]:
                print("  Status: Operational")

        # Provide recovery suggestions for unhealthy services
        for service_name, status in health_status["services"].items():
            if not status["healthy"] and "error" in status:
                suggestion = ErrorHandler.get_recovery_suggestion(
                    {"error": status["error"], "details": {"category": "service_error"}}
                )
                if suggestion:
                    print(f"\n💡 {service_name.title()} fix: {suggestion}")

    except Exception as e:
        print(f"❌ Health check failed: {e}")
        if verbose:
            import traceback
            print(traceback.format_exc())

    print("\n✅ Health check complete")


def info_command():
    """Display system information using new services"""
    print("📊 System Information")
    print("=" * 50)

    # Database stats
    try:
        db = SimpleDB()
        stats = db.get_content_stats()
        print("\n📁 Database Statistics:")
        print(f"  Gmail emails: {stats.get('total_emails', 0)} (deduplicated messages)")
        print(f"  PDFs: {stats.get('total_pdfs', 0)}")
        print(f"  Transcripts: {stats.get('total_transcripts', 0)}")
        print(f"  Searchable content pieces: {stats.get('total_content', 0)}")
        print(f"  Note: Content pieces may include parsed email parts - Gmail sync shows actual email count")
    except Exception as e:
        print(f"  ⚠️  Database unavailable: {e}")

    # Vector service status - Direct > Indirect
    try:
        from utilities.vector_store import get_vector_store

        store = get_vector_store()
        print("\n🧠 Vector Service:")
        print("  Status: ✅ Connected")
        print(f"  Collection: {store.collection}")
        print(f"  Dimensions: {store.dimensions}")
    except Exception as e:
        print("\n🧠 Vector Service:")
        print(f"  Status: ❌ Not available ({e})")

    # Embeddings status - Direct > Indirect
    try:
        from utilities.embeddings import get_embedding_service

        emb = get_embedding_service()
        print("\n🤖 Embedding Service:")
        print(f"  Model: {emb.model_name}")
        print(f"  Dimensions: {emb.dimensions}")
        print(f"  Device: {emb.device}")
    except Exception as e:
        print("\n🤖 Embedding Service:")
        print(f"  Status: ⚠️  Not configured ({e})")

    print("\n✅ System check complete")


def upload_command(file_path):
    """Upload and process a document (PDF only)."""
    from pathlib import Path

    file_path = Path(file_path)
    if not file_path.exists():
        print(f"❌ File not found: {file_path}")
        return False

    print(f"📤 Uploading: {file_path.name}")

    try:
        # Handle based on file type
        if file_path.suffix.lower() == ".pdf":
            # Use the new upload_handler for PDF files
            from tools.scripts.cli.upload_handler import upload_pdf

            success = upload_pdf(str(file_path), source="vsearch")
            return success
        else:
            # Other file types not supported after analog removal
            print(f"❌ Unsupported file type: {file_path.suffix}")
            print("💡 Supported types: PDF")
            return False

    except Exception as e:
        print(f"❌ Upload failed: {e}")
        return False


def chunk_ingest_command(args):
    """Process documents through v2 chunking pipeline."""
    import json
    from utilities.chunk_pipeline import ChunkPipeline
    from utilities.embeddings.batch_processor import BatchEmbeddingProcessor
    
    # Show statistics if requested
    if args.stats:
        pipeline = ChunkPipeline(quality_threshold=args.quality_threshold)
        processor = BatchEmbeddingProcessor(
            batch_size=args.batch_size,
            min_quality=args.quality_threshold
        )
        
        # Get statistics
        chunk_stats = pipeline.get_pipeline_stats()
        embed_stats = processor.get_processor_stats()
        
        # Combine stats
        stats = {**chunk_stats, **embed_stats}
        
        if args.json:
            print(json.dumps(stats, indent=2))
        else:
            print("\n📊 Chunk Pipeline Statistics")
            print("=" * 60)
            print(f"Documents ready for chunking: {stats.get('documents_ready', 0)}")
            print(f"Documents already chunked: {stats.get('documents_chunked', 0)}")
            print(f"Total chunks: {stats.get('total_chunks', 0)}")
            print(f"  - High quality (>={args.quality_threshold}): {stats.get('high_quality_chunks', 0)}")
            print(f"  - Low quality (<{args.quality_threshold}): {stats.get('low_quality_chunks', 0)}")
            print(f"Chunks ready for embedding: {stats.get('chunks_ready', 0)}")
            print(f"Chunks already embedded: {stats.get('chunks_embedded', 0)}")
            print(f"Vectors in Qdrant (vectors_v2): {stats.get('vectors_in_qdrant', 'unknown')}")
            
            if stats.get('chunks_ready', 0) > 0:
                print(f"\n⏱️  Estimated time to embed remaining: {stats.get('estimated_minutes', '?')} minutes")
        return
    
    print("🔄 Starting v2 chunk ingestion pipeline...")
    
    if args.dry_run:
        print("🔍 DRY RUN MODE - No changes will be made")
    
    # Phase 1: Chunking
    print("\n📄 Phase 1: Document Chunking")
    print("-" * 40)
    
    try:
        pipeline = ChunkPipeline(quality_threshold=args.quality_threshold)
        chunk_result = pipeline.process_documents(
            limit=args.limit,
            dry_run=args.dry_run
        )
        
        if args.json:
            print(json.dumps(chunk_result, indent=2, default=str))
        else:
            print(f"✅ Documents processed: {chunk_result['documents_processed']}")
            print(f"✅ Chunks created: {chunk_result['chunks_created']}")
            print(f"⚠️  Chunks dropped (quality < {args.quality_threshold}): {chunk_result['chunks_dropped_quality']}")
            print(f"⏭️  Chunks skipped (already exist): {chunk_result['chunks_already_exist']}")
            print(f"⏱️  Time: {chunk_result['elapsed_seconds']:.1f}s")
            
            if chunk_result["errors"]:
                print(f"\n❌ Errors ({len(chunk_result['errors'])})")
                for error in chunk_result["errors"][:5]:
                    print(f"  - {error}")
    
    except Exception as e:
        print(f"❌ Chunking failed: {e}")
        return False
    
    # Phase 2: Embedding Generation (optional)
    if args.embeddings and not args.dry_run:
        print("\n🧠 Phase 2: Embedding Generation")
        print("-" * 40)
        
        try:
            processor = BatchEmbeddingProcessor(
                batch_size=args.batch_size,
                min_quality=args.quality_threshold
            )
            
            # Process chunks that were just created
            embed_result = processor.process_chunks(
                limit=chunk_result.get('chunks_created', 0) if args.limit else None,
                dry_run=False
            )
            
            if args.json:
                print(json.dumps(embed_result, indent=2, default=str))
            else:
                print(f"✅ Chunks processed: {embed_result['chunks_processed']}")
                print(f"✅ Embeddings generated: {embed_result['embeddings_generated']}")
                print(f"✅ Vectors stored in Qdrant: {embed_result['vectors_stored']}")
                print(f"⏱️  Time: {embed_result['elapsed_seconds']:.1f}s")
                
                if embed_result.get('embeddings_per_second'):
                    print(f"⚡ Rate: {embed_result['embeddings_per_second']:.1f} embeddings/sec")
                
                if embed_result["errors"]:
                    print(f"\n❌ Errors ({len(embed_result['errors'])})")
                    for error in embed_result["errors"][:5]:
                        print(f"  - {error}")
        
        except Exception as e:
            print(f"❌ Embedding generation failed: {e}")
            return False
    
    print("\n✅ Chunk ingestion pipeline complete!")
    return True

def ingest_command(args):
    """Unified ingestion command for emails and/or documents."""
    from shared.unified_ingestion import get_ingestion_service

    # Determine what to process
    process_emails = args.emails or args.all or (not args.emails and not args.docs and not args.all)
    process_docs = args.docs or args.all or (not args.emails and not args.docs and not args.all)
    
    print("🔄 Starting unified ingestion...")
    
    try:
        ingestion_service = get_ingestion_service()
        
        if process_emails and process_docs:
            # Process both
            result = ingestion_service.ingest_all(
                document_directory=args.dir,
                email_since=args.since
            )
            
            # Display results
            email_results = result.get("emails", {})
            doc_results = result.get("documents", {})
            
            print(f"\n📧 Email Results:")
            print(f"  Processed: {email_results.get('processed', 0)}")
            if email_results.get('errors', 0) > 0:
                print(f"  Errors: {email_results.get('errors', 0)}")
            
            print(f"\n📄 Document Results:")
            print(f"  Processed: {doc_results.get('processed', 0)}")
            if doc_results.get('errors', 0) > 0:
                print(f"  Errors: {doc_results.get('errors', 0)}")
            
            print(f"\n✅ Total Processed: {result.get('totals', {}).get('processed', 0)}")
            
        elif process_emails:
            # Process emails only
            result = ingestion_service.ingest_emails(since=args.since)
            
            print(f"\n📧 Email Results:")
            print(f"  Processed: {result.get('processed', 0)}")
            if result.get('errors', 0) > 0:
                print(f"  Errors: {result.get('errors', 0)}")
            
        elif process_docs:
            # Process documents only
            result = ingestion_service.ingest_documents(directory=args.dir)
            
            print(f"\n📄 Document Results:")
            print(f"  Processed: {result.get('processed', 0)}")
            if result.get('errors', 0) > 0:
                print(f"  Errors: {result.get('errors', 0)}")
        
        if result.get("success", False):
            print(f"\n✅ Ingestion completed successfully")
            return True
        else:
            print(f"\n❌ Ingestion failed: {result.get('error', 'Unknown error')}")
            return False
            
    except Exception as e:
        print(f"❌ Ingestion error: {e}")
        return False


def _setup_argument_parser() -> argparse.ArgumentParser:
    """Set up and configure the main argument parser."""
    parser = argparse.ArgumentParser(
        description="""AI-Powered Database Search CLI

Search across database content with Legal BERT embeddings.
System operates on SQLite database with 52+ documents.

Examples:
  # Basic search
  vsearch search "contract amendment"
  
  # Advanced search with filters
  vsearch search "settlement" --since "last month" --type email --verbose
  vsearch search "contract" --tag legal --tag draft --tag-logic AND
  
  # Legal Intelligence
  vsearch legal timeline "24NNCV"        # Generate case timeline
  vsearch legal search "discovery"       # Legal-specific search
  
  # Search Intelligence
  vsearch intelligence smart-search "contract"   # Query expansion search
  vsearch intelligence cluster --threshold 0.8   # Find document clusters
  
  # Legal Evidence Tracking
  vsearch evidence status                     # Check evidence tracking status
  vsearch evidence assign-eids                # Assign Evidence IDs to emails
  vsearch evidence assign-threads             # Group emails into threads
  vsearch evidence report --keywords "entry"  # Generate evidence reports
  
  # System information
  vsearch info          # Show system status
  vsearch health -v     # Detailed health check
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    # Global warmup flag
    parser.add_argument(
        "--warmup", action="store_true", 
        help="Preload models and services before executing command"
    )

    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    
    # Warmup command
    warmup_parser = subparsers.add_parser(
        "warmup", help="Preload models and services for faster subsequent commands"
    )

    # Search command
    search_parser = subparsers.add_parser(
        "search", help="Search content with AI-powered semantic search"
    )
    search_parser.add_argument(
        "query", help="Natural language search query"
    )
    search_parser.add_argument("-n", "--limit", type=int, default=5, help="Number of results")
    search_parser.add_argument(
        "--keyword-only", action="store_true", help="Use keyword search only"
    )
    search_parser.add_argument(
        "--semantic-only", action="store_true", help="Use semantic search only"
    )
    search_parser.add_argument(
        "--chunk-search", action="store_true", help="Enable chunk-level search with document aggregation"
    )
    search_parser.add_argument(
        "--no-chunks", action="store_true", help="Disable chunk aggregation (use document-level search)"
    )
    search_parser.add_argument("-v", "--verbose", action="store_true", help="Show detailed errors")
    # Date filtering
    search_parser.add_argument(
        "--since", help="Start date (e.g., '2024-01-01', 'last week', '3 days ago')"
    )
    search_parser.add_argument(
        "--until", help="End date (e.g., '2024-12-31', 'yesterday', 'this month')"
    )
    # Content type filtering
    search_parser.add_argument(
        "--type",
        "--types",
        dest="types",
        action="append",
        help="Content types to search (email, pdf, transcript, note). Can be used multiple times.",
    )
    # Tag filtering
    search_parser.add_argument(
        "--tag",
        "--tags",
        dest="tags",
        action="append",
        help="Tags to search for. Can be used multiple times.",
    )
    search_parser.add_argument(
        "--tag-logic",
        choices=["AND", "OR"],
        default="OR",
        help="Logic for combining multiple tags (default: OR)",
    )

    # Info command
    subparsers.add_parser("info", help="Display system information")

    # Health command
    health_parser = subparsers.add_parser("health", help="Check system health")
    health_parser.add_argument("-v", "--verbose", action="store_true", help="Show detailed status")

    # Upload command
    upload_parser = subparsers.add_parser("upload", help="Upload and process a document")
    upload_parser.add_argument("file", help="Path to file to upload")

    # Ingest command
    ingest_parser = subparsers.add_parser("ingest", help="Ingest emails and/or documents through unified pipeline")
    ingest_group = ingest_parser.add_mutually_exclusive_group()
    ingest_group.add_argument("--emails", action="store_true", help="Process emails only")
    ingest_group.add_argument("--docs", action="store_true", help="Process documents only")
    ingest_group.add_argument("--all", action="store_true", help="Process both emails and documents (default)")
    ingest_parser.add_argument("--dir", default="data/Stoneman_dispute/user_data", 
                              help="Directory for document processing (default: data/Stoneman_dispute/user_data)")
    ingest_parser.add_argument("--since", help="Date filter for emails (e.g., 'last week', '2024-01-01')")

    # Chunk ingestion (v2 pipeline) command
    chunk_ingest_parser = subparsers.add_parser(
        "chunk-ingest", 
        help="Process documents through v2 chunking pipeline with quality scoring"
    )
    chunk_ingest_parser.add_argument("--batch-size", type=int, default=64, help="Number of chunks per embedding batch (default: 64)")
    chunk_ingest_parser.add_argument("--quality-threshold", type=float, default=0.35, help="Minimum quality score (default: 0.35)")
    chunk_ingest_parser.add_argument("--limit", type=int, help="Maximum documents to process")
    chunk_ingest_parser.add_argument("--dry-run", action="store_true", help="Preview without processing")
    chunk_ingest_parser.add_argument("--embeddings", action="store_true", help="Also generate embeddings for chunks")
    chunk_ingest_parser.add_argument("--stats", action="store_true", help="Show pipeline statistics")
    chunk_ingest_parser.add_argument("--json", action="store_true", help="Output as JSON")

    # Legal Intelligence commands
    legal_parser = subparsers.add_parser("legal", help="Legal Intelligence analysis tools")
    legal_subparsers = legal_parser.add_subparsers(dest="legal_command", help="Legal commands")

    # Process command
    process_parser = legal_subparsers.add_parser("process", help="Process a legal case")
    process_parser.add_argument("case_id", help="Case identifier or search pattern")
    process_parser.add_argument(
        "--format", choices=["text", "json"], default="text", help="Output format (default: text)"
    )

    # Timeline command
    timeline_parser = legal_subparsers.add_parser("timeline", help="Generate case timeline")
    timeline_parser.add_argument("case_id", help="Case identifier or search pattern")
    timeline_parser.add_argument("-o", "--output", help="Output file path (optional)")

    # Graph command
    graph_parser = legal_subparsers.add_parser("graph", help="Build relationship graph")
    graph_parser.add_argument("case_id", help="Case identifier or search pattern")
    graph_parser.add_argument(
        "--depth", type=int, default=3, help="Maximum relationship depth (default: 3)"
    )

    # Search command
    legal_search_parser = legal_subparsers.add_parser("search", help="Legal-specific search")
    legal_search_parser.add_argument("query", help="Search query")
    legal_search_parser.add_argument("--case", help="Filter by case ID")
    legal_search_parser.add_argument(
        "-n", "--limit", type=int, default=10, help="Number of results (default: 10)"
    )

    # Missing documents command
    missing_parser = legal_subparsers.add_parser("missing", help="Predict missing documents")
    missing_parser.add_argument("case_id", help="Case identifier or search pattern")
    missing_parser.add_argument(
        "--confidence", type=float, default=0.6, help="Confidence threshold (default: 0.6)"
    )

    # Summarize command
    summarize_parser = legal_subparsers.add_parser("summarize", help="Summarize legal documents")
    summarize_parser.add_argument("case_id", help="Case identifier or search pattern")
    summarize_parser.add_argument(
        "--max-docs", type=int, default=10, help="Maximum documents to summarize (default: 10)"
    )

    # Search Intelligence commands
    intelligence_parser = subparsers.add_parser(
        "intelligence", help="Search Intelligence analysis tools"
    )
    intel_subparsers = intelligence_parser.add_subparsers(
        dest="intel_command", help="Intelligence commands"
    )

    # Smart search command
    smart_search_parser = intel_subparsers.add_parser(
        "smart-search", help="Intelligent search with preprocessing"
    )
    smart_search_parser.add_argument("query", help="Search query")
    smart_search_parser.add_argument(
        "-n", "--limit", type=int, default=10, help="Number of results (default: 10)"
    )
    smart_search_parser.add_argument(
        "--no-expansion", action="store_true", help="Disable query expansion"
    )
    smart_search_parser.add_argument("--json", action="store_true", help="Output as JSON")

    # Similarity command
    similarity_parser = intel_subparsers.add_parser("similarity", help="Find similar documents")
    similarity_parser.add_argument("doc_id", help="Document ID to find similarities for")
    similarity_parser.add_argument(
        "-n", "--limit", type=int, default=10, help="Number of similar documents (default: 10)"
    )
    similarity_parser.add_argument(
        "--threshold", type=float, default=0.7, help="Similarity threshold (default: 0.7)"
    )
    similarity_parser.add_argument("--json", action="store_true", help="Output as JSON")

    # Cluster command
    cluster_parser = intel_subparsers.add_parser("cluster", help="Cluster similar content")
    cluster_parser.add_argument(
        "--threshold", type=float, default=0.7, help="Similarity threshold (default: 0.7)"
    )
    cluster_parser.add_argument("--content-type", help="Filter by content type (email, pdf, etc.)")
    cluster_parser.add_argument(
        "-n", "--limit", type=int, default=100, help="Maximum documents to cluster (default: 100)"
    )
    cluster_parser.add_argument(
        "--min-samples", type=int, default=2, help="Minimum samples for cluster (default: 2)"
    )
    cluster_parser.add_argument("--json", action="store_true", help="Output as JSON")

    # Duplicates command
    duplicates_parser = intel_subparsers.add_parser("duplicates", help="Detect duplicate documents")
    duplicates_parser.add_argument("--content-type", help="Filter by content type")
    duplicates_parser.add_argument(
        "--threshold",
        type=float,
        default=0.95,
        help="Similarity threshold for near-duplicates (default: 0.95)",
    )
    duplicates_parser.add_argument("--json", action="store_true", help="Output as JSON")

    # Entities command
    entities_parser = intel_subparsers.add_parser("entities", help="Extract and cache entities")
    entities_parser.add_argument("doc_id", help="Document ID to extract entities from")
    entities_parser.add_argument(
        "--force-refresh", action="store_true", help="Force re-extraction even if cached"
    )
    entities_parser.add_argument("--json", action="store_true", help="Output as JSON")

    # Summarize command (intelligence version)
    intel_summarize_parser = intel_subparsers.add_parser(
        "summarize", help="Auto-summarize document"
    )
    intel_summarize_parser.add_argument("doc_id", help="Document ID to summarize")
    intel_summarize_parser.add_argument(
        "--sentences", type=int, default=3, help="Maximum sentences (default: 3)"
    )
    intel_summarize_parser.add_argument(
        "--keywords", type=int, default=10, help="Maximum keywords (default: 10)"
    )
    intel_summarize_parser.add_argument(
        "--no-cache", action="store_true", help="Don't cache the summary"
    )
    intel_summarize_parser.add_argument("--json", action="store_true", help="Output as JSON")

    # Evidence commands
    evidence_parser = subparsers.add_parser("evidence", help="Legal evidence tracking and reports")
    evidence_subparsers = evidence_parser.add_subparsers(dest="evidence_command", help="Evidence commands")
    
    # Assign EIDs command
    assign_eids_parser = evidence_subparsers.add_parser("assign-eids", help="Assign Evidence IDs to emails")
    assign_eids_parser.add_argument("--limit", type=int, help="Maximum emails to process")
    
    # Assign threads command
    evidence_subparsers.add_parser("assign-threads", help="Group emails into conversation threads")
    
    # Lookup command
    lookup_parser = evidence_subparsers.add_parser("lookup", help="Look up specific evidence")
    lookup_parser.add_argument("--eid", help="Evidence ID to look up")
    lookup_parser.add_argument("--thread", help="Thread ID to look up")
    
    # Report command
    report_parser = evidence_subparsers.add_parser("report", help="Generate legal evidence reports")
    report_parser.add_argument("-o", "--output", default="legal_evidence_export", help="Output directory")
    report_parser.add_argument("--keywords", nargs="+", help="Keywords to search for (comma-separated or multiple)")
    report_parser.add_argument("--threads", nargs="+", help="Specific thread IDs to include")
    report_parser.add_argument("--mode", choices=["lookup", "narrative", "both", "export"], 
                              default="both", help="Report mode (default: both)")
    
    # Search pattern command
    pattern_parser = evidence_subparsers.add_parser("search", help="Search for patterns in emails")
    pattern_parser.add_argument("pattern", help="Pattern to search for")
    pattern_parser.add_argument("-n", "--limit", type=int, default=100, help="Maximum results")
    
    # Analyze thread command
    analyze_parser = evidence_subparsers.add_parser("analyze", help="Analyze a thread for patterns")
    analyze_parser.add_argument("thread_id", help="Thread ID to analyze")
    analyze_parser.add_argument("--format", choices=["text", "json"], default="text", help="Output format")
    
    # Status command
    evidence_subparsers.add_parser("status", help="Show evidence tracking status")

    # Semantic backfill command
    semantic_parser = subparsers.add_parser("semantic", help="Semantic enrichment operations")
    semantic_subparsers = semantic_parser.add_subparsers(dest="semantic_command", help="Semantic commands")
    
    # Backfill command
    backfill_parser = semantic_subparsers.add_parser("backfill", help="Backfill semantic enrichment for old emails")
    backfill_parser.add_argument("--steps", nargs="+", choices=["entities", "embeddings", "timeline"],
                                help="Specific steps to run (default: all)")
    backfill_parser.add_argument("--batch-size", type=int, default=50, help="Batch size (default: 50)")
    backfill_parser.add_argument("--limit", type=int, help="Maximum emails to process")
    backfill_parser.add_argument("--force", action="store_true", help="Force reprocessing")
    backfill_parser.add_argument("--since-days", type=int, help="Only process emails from last N days")
    
    # Status command
    semantic_subparsers.add_parser("status", help="Show semantic enrichment status")
    
    # Verify command
    verify_parser = semantic_subparsers.add_parser("verify", help="Verify semantic pipeline wiring")
    verify_parser.add_argument("--quick", action="store_true", help="Quick check only")
    
    # Lookup command
    lookup_parser = semantic_subparsers.add_parser("lookup", help="EID-first evidence lookup")
    lookup_parser.add_argument("--eid", required=True, help="Evidence ID to lookup")

    # Add quarantine commands
    add_quarantine_commands(subparsers)
    
    return parser


def _build_search_filters(args) -> dict | None:
    """Build filters dictionary from command line arguments."""
    filters = {}
    if args.since:
        filters["since"] = args.since
    if args.until:
        filters["until"] = args.until
    if args.types:
        filters["content_types"] = args.types
    if args.tags:
        filters["tags"] = args.tags
        filters["tag_logic"] = args.tag_logic
    
    # Return filters only if any are specified
    return filters if filters else None


def _dispatch_command(args) -> None:
    """Dispatch to appropriate command handler based on parsed arguments."""
    # Default to search if no command but arguments provided
    if not args.command and len(sys.argv) > 1:
        # Treat first argument as search query
        query = " ".join(sys.argv[1:])
        search_command(query)
    elif args.command == "search":
        filters = _build_search_filters(args)
        
        # Determine search mode based on flags
        search_mode = "hybrid"  # default
        if args.keyword_only and args.semantic_only:
            print("❌ Cannot use both --keyword-only and --semantic-only flags")
            return
        elif args.keyword_only:
            search_mode = "keyword"
        elif args.semantic_only:
            search_mode = "semantic"
        
        # Determine chunk aggregation mode
        use_chunk_aggregation = None  # Let system use default
        if hasattr(args, 'chunk_search') and args.chunk_search:
            use_chunk_aggregation = True
        elif hasattr(args, 'no_chunks') and args.no_chunks:
            use_chunk_aggregation = False
        
        search_command(
            args.query,
            args.limit,
            verbose=args.verbose,
            filters=filters,
            search_mode=search_mode,
            use_chunk_aggregation=use_chunk_aggregation,
        )
    elif args.command == "warmup":
        warmup_command()
    elif args.command == "info":
        info_command()
    elif args.command == "health":
        health_command(verbose=args.verbose)
    elif args.command == "upload":
        upload_command(args.file)
    elif args.command == "ingest":
        ingest_command(args)
    elif args.command == "chunk-ingest":
        chunk_ingest_command(args)
    elif args.command == "legal":
        # Import legal handlers
        from tools.scripts.cli.legal_handler import (
            build_legal_graph,
            generate_legal_timeline,
            predict_missing_documents,
            process_legal_case,
            search_legal,
            summarize_legal_docs,
        )

        if args.legal_command == "process":
            process_legal_case(args.case_id, args.format)
        elif args.legal_command == "timeline":
            generate_legal_timeline(args.case_id, args.output)
        elif args.legal_command == "graph":
            build_legal_graph(args.case_id, args.depth)
        elif args.legal_command == "search":
            search_legal(args.query, args.case, args.limit)
        elif args.legal_command == "missing":
            predict_missing_documents(args.case_id, args.confidence)
        elif args.legal_command == "summarize":
            summarize_legal_docs(args.case_id, args.max_docs)
        else:
            legal_parser.print_help()
    elif args.command == "intelligence":
        # Import intelligence handlers
        from tools.scripts.cli.intelligence_handler import (
            cluster_command,
            duplicates_command,
            entities_command,
            intel_summarize_command,
            similarity_command,
            smart_search_command,
        )

        if args.intel_command == "smart-search":
            smart_search_command(args.query, args.limit, not args.no_expansion, args.json)
        elif args.intel_command == "similarity":
            similarity_command(args.doc_id, args.limit, args.threshold, args.json)
        elif args.intel_command == "cluster":
            cluster_command(
                args.threshold, args.content_type, args.limit, args.min_samples, args.json
            )
        elif args.intel_command == "duplicates":
            duplicates_command(args.content_type, args.threshold, args.json)
        elif args.intel_command == "entities":
            entities_command(args.doc_id, args.force_refresh, args.json)
        elif args.intel_command == "summarize":
            intel_summarize_command(
                args.doc_id, args.sentences, args.keywords, not args.no_cache, args.json
            )
        else:
            intelligence_parser.print_help()
    elif args.command == "evidence":
        # Import evidence handlers
        from tools.cli.evidence_handler import (
            analyze_thread_command,
            assign_eids_command,
            assign_threads_command,
            lookup_command,
            report_command,
            search_pattern_command,
            status_command,
        )
        
        if args.evidence_command == "assign-eids":
            assign_eids_command(args.limit)
        elif args.evidence_command == "assign-threads":
            assign_threads_command()
        elif args.evidence_command == "lookup":
            lookup_command(args.eid, args.thread)
        elif args.evidence_command == "report":
            report_command(args.output, args.keywords, args.threads, args.mode)
        elif args.evidence_command == "search":
            search_pattern_command(args.pattern, args.limit)
        elif args.evidence_command == "analyze":
            analyze_thread_command(args.thread_id, args.format)
        elif args.evidence_command == "status":
            status_command()
        else:
            evidence_parser.print_help()
    elif args.command == "semantic":
        # Import semantic handlers
        from scripts.backfill_semantic import backfill_semantic
        from scripts.verify_semantic_wiring import SemanticWiringVerifier, eid_lookup
        from shared.simple_db import SimpleDB
        
        if args.semantic_command == "backfill":
            backfill_semantic(
                steps=args.steps,
                batch_size=args.batch_size,
                limit=args.limit,
                force=args.force,
                since_days=args.since_days
            )
        elif args.semantic_command == "status":
            # Show semantic enrichment status
            db = SimpleDB()
            print("=" * 60)
            print("Semantic Enrichment Status")
            print("=" * 60)
            
            cursor = db.execute("SELECT COUNT(*) FROM emails")
            total_emails = cursor.fetchone()[0]
            print(f"\nTotal emails: {total_emails}")
            
            cursor = db.execute("SELECT COUNT(*) FROM entity_content_mapping")
            entity_count = cursor.fetchone()[0]
            print(f"Entities extracted: {entity_count}")
            
            cursor = db.execute("SELECT COUNT(*) FROM content_unified WHERE embedding_generated = 1")
            vector_count = cursor.fetchone()[0]
            print(f"Vectorized content: {vector_count}")
            
            cursor = db.execute("SELECT COUNT(*) FROM timeline_events")
            timeline_count = cursor.fetchone()[0]
            print(f"Timeline events: {timeline_count}")
            
            # EID integration status (simplified check)
            print(f"\nEID Integration:")
            print(f"  System ready for EID tracking")
        elif args.semantic_command == "verify":
            # Run comprehensive wiring verification
            verifier = SemanticWiringVerifier()
            if args.quick:
                verifier.verify_end_to_end()
            else:
                verifier.run_all_checks()
        elif args.semantic_command == "lookup":
            # EID-first lookup
            if args.eid:
                eid_lookup(args.eid)
            else:
                print("Error: --eid required for lookup command")
        else:
            semantic_parser.print_help()
    elif args.command == "quarantine":
        handle_quarantine_command(args)
    else:
        parser.print_help()


def main():
    """Main CLI entry point - simplified orchestrator."""
    parser = _setup_argument_parser()
    args = parser.parse_args()
    
    # Handle global warmup flag
    if hasattr(args, 'warmup') and args.warmup:
        print("🚀 Global warmup requested...")
        warmup_command()
        print()  # Add spacing before main command
    
    _dispatch_command(args)


if __name__ == "__main__":
    main()