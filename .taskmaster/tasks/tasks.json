{
  "master": {
    "tasks": [
      {
        "id": 19,
        "title": "Implement Gmail API Rate Limit Handling",
        "description": "Add robust rate limit handling and retry logic for Gmail API operations to prevent failures during high-volume email sync operations",
        "details": "Implement exponential backoff retry strategy with jitter for Gmail API rate limits. Add circuit breaker pattern to prevent cascading failures. Track API quota usage and implement adaptive throttling. Use the existing retry_helper.py utilities and extend with Gmail-specific rate limit detection (403/429 status codes). Store rate limit state in SimpleDB to persist across sessions. Include metrics for monitoring API usage patterns.",
        "testStrategy": "Unit tests for retry logic with mocked API responses. Integration tests simulating rate limit scenarios. Performance tests verifying throttling behavior under load. Monitor actual Gmail API usage patterns in production.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Enhance Gmail Sender Filtering",
        "description": "Improve email relevancy by adding configurable sender filters and domain whitelisting/blacklisting capabilities",
        "details": "Extend GmailConfig to support sender filters with regex patterns, domain lists, and importance scoring. Implement filter configuration in YAML format stored in .taskmaster/config/gmail_filters.yaml. Add ML-based sender importance detection using existing entity extraction. Create sender reputation scoring based on email frequency and user interaction patterns. Store sender metadata in SimpleDB for persistence.",
        "testStrategy": "Unit tests for filter matching logic. Integration tests with sample email datasets. Validate filter performance with real email corpus. Test configuration loading and validation.",
        "priority": "medium",
        "dependencies": [
          19
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 21,
        "title": "Optimize PDF OCR Image Preprocessing",
        "description": "Improve OCR accuracy by enhancing image preprocessing pipeline with advanced computer vision techniques",
        "details": "Enhance pdf/ocr/page_processor.py with adaptive thresholding, deskewing, and noise reduction using OpenCV. Implement text region detection using EAST or CRAFT models. Add image quality assessment to determine optimal preprocessing parameters. Use parallel processing for multi-page PDFs. Cache preprocessed images to avoid redundant processing. Integrate with existing OCR coordinator for seamless operation.",
        "testStrategy": "Benchmark OCR accuracy on test dataset of scanned legal documents. Compare accuracy before/after preprocessing improvements. Test with various document qualities (low DPI, skewed, noisy). Validate memory usage stays under 50MB per operation.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 22,
        "title": "Add Legal Document Format Detection",
        "description": "Implement intelligent detection of legal document formats and structures for better metadata extraction",
        "details": "Create legal document classifier using regex patterns and SpaCy NER for common formats (motions, pleadings, contracts, discovery). Extract structured metadata (case numbers, parties, filing dates) using pattern matching. Store document type taxonomy in SimpleDB. Integrate with existing legal_intelligence service for enhanced analysis. Add support for jurisdiction-specific formats.",
        "testStrategy": "Test against corpus of known legal document types. Validate extraction accuracy for key metadata fields. Test with documents from multiple jurisdictions. Integration tests with legal_intelligence service.",
        "priority": "medium",
        "dependencies": [
          21
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 23,
        "title": "Implement Search Query Expansion Rules",
        "description": "Enhance search intelligence with domain-specific query expansion for legal and business contexts",
        "details": "Extend search_intelligence/main.py with configurable query expansion rules. Add legal synonym dictionary (plaintiff/petitioner, defendant/respondent). Implement abbreviation expansion (LLC, Corp, Inc). Use WordNet for general synonym expansion. Store expansion rules in JSON configuration. Add context-aware expansion based on document type. Cache expanded queries for performance.",
        "testStrategy": "Unit tests for expansion logic with legal terms. Integration tests measuring search recall improvements. A/B testing comparing search results with/without expansion. Validate performance impact stays under 2 second threshold.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 24,
        "title": "Auto-tune Similarity Thresholds",
        "description": "Implement adaptive similarity threshold tuning based on document characteristics and user feedback",
        "details": "Create feedback loop to track search result relevance. Implement threshold adjustment algorithm using DBSCAN clustering metrics. Store per-document-type optimal thresholds in SimpleDB. Add A/B testing framework for threshold experiments. Use statistical analysis to determine optimal thresholds per content type. Integrate with existing duplicate_detector.py for consistency.",
        "testStrategy": "Test threshold adaptation with synthetic feedback data. Validate clustering quality metrics improve over time. Integration tests with real search queries. Monitor false positive/negative rates.",
        "priority": "low",
        "dependencies": [
          23
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 25,
        "title": "Improve CLI Progress Indicators",
        "description": "Add rich progress bars and status updates for long-running operations in vsearch CLI",
        "details": "Integrate Rich library for beautiful terminal output (already used in docs/examples/rich_cli_demo.py). Add progress bars for batch operations (PDF processing, email sync, transcription). Implement live status updates using Rich's Live display. Add spinner animations for indeterminate operations. Create consistent progress reporting API across all services. Maintain fallback to simple output for non-TTY environments.",
        "testStrategy": "Test progress display in various terminal environments. Validate output in CI/CD pipelines (non-TTY). Test with operations of varying durations. Ensure no performance impact from progress updates.",
        "priority": "low",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 26,
        "title": "Enhance Error Messages",
        "description": "Improve user-facing error messages with actionable recovery suggestions and context",
        "details": "Extend shared/error_handler.py with more specific error categories and recovery suggestions. Add error code system for common failures. Implement contextual help based on operation type. Create error message templates with placeholders for dynamic content. Add 'did you mean' suggestions for common mistakes. Log detailed errors while showing simplified messages to users.",
        "testStrategy": "Test error messages for all common failure scenarios. Validate recovery suggestions actually resolve issues. User testing for message clarity. Test error logging captures sufficient debug information.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 27,
        "title": "Add DOCX Document Support",
        "description": "Implement DOCX file processing using existing document processor framework",
        "details": "Extend infrastructure/documents/processors/docx_processor.py (already exists but needs implementation). Use python-docx library for text extraction. Extract document metadata (author, creation date, revision history). Handle embedded images and tables. Convert formatting to markdown for storage. Integrate with existing pipeline orchestrator. Add DOCX to supported formats in vsearch upload command.",
        "testStrategy": "Test with various DOCX formats (Office 365, older Office versions). Validate text extraction completeness. Test handling of complex documents (tables, images, styles). Integration tests with upload pipeline.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 28,
        "title": "Profile Memory Usage",
        "description": "Implement memory profiling and optimization for large dataset operations",
        "details": "Add memory profiling using memory_profiler and tracemalloc. Create memory usage benchmarks for key operations (PDF processing, embedding generation, batch email sync). Identify and fix memory leaks in long-running processes. Implement streaming/chunking for large file processing. Add memory usage monitoring to health check system. Set up alerts for memory threshold violations (>50MB per operation target).",
        "testStrategy": "Run memory profiler on all major operations. Test with large datasets (1000+ documents). Validate memory stays under limits during batch operations. Stress test with concurrent operations.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-08-14T20:57:32.199Z",
      "updated": "2025-08-19T00:20:26.506Z",
      "description": "Tasks for master context"
    }
  },
  "import-cleanup": {
    "tasks": [
      {
        "id": 1,
        "title": "Repair Broken Import Paths for shared.simple_db and Other Modules",
        "description": "Identify and correct 107 failing import statements across the codebase, starting with the 48 modules that incorrectly reference shared.simple_db.",
        "details": "1. Inventory & Prioritisation:\n   • Generate a list of all ImportError stack traces from the CI logs and group by module path.\n   • Tag the 48 files referencing shared.simple_db as P1; the remaining 59 files as P2.\n\n2. Root-Cause Analysis:\n   • Confirm the canonical location of simple_db (e.g. src/shared/simple_db or src/core/db/simple_db).\n   • Check whether the package has been renamed, relocated, or improperly initialised in __init__.py files.\n\n3. Automated Refactor:\n   • Write a one-off Python script that walks the repository (Pathlib + ast) and rewrites from … import … and import … lines that reference the old path.\n   • Keep a mapping dictionary such as {\n       \"shared.simple_db\": \"shared.db.simple_db\",\n       \"utils.helpers\": \"shared.utils.helpers\",\n       …\n     } so the same script can fix remaining import errors.\n   • Ensure the script preserves relative/absolute imports and formatting via lib2to3 or astor.\n\n4. Manual Edge-Cases:\n   • Review any files the script skips (dynamic imports, __all__ overrides, try/except ImportError blocks) and patch by hand.\n   • Add or update __init__.py where implicit namespace packages are causing look-ups to fail.\n\n5. Lint & Build:\n   • Run mypy/flake8 to ensure no unresolved references remain.\n   • Re-run the entire test suite and CI build; iterate until green.\n\n6. Regression Protection:\n   • Add import sanity check to pre-commit (python -m pip checkimports) so new PRs cannot introduce broken imports again.\n   • Document path conventions in CONTRIBUTING.md.\n\nConsiderations:\n• Maintain backward compatibility by leaving deprecated re-export stubs (e.g. in shared/__init__.py) that raise deprecation warnings but do not break existing callers.\n• If simple_db uses relative imports internally, verify those still resolve after relocation.\n• Keep the refactor in its own PR for ease of review and rollback.",
        "testStrategy": "1. Unit & Integration Tests\n   • Execute pytest ‑m \"not slow\" locally; expect zero ImportErrors.\n   • Run the full CI pipeline; build must pass.\n\n2. Static Analysis\n   • Run python -m pip install .[dev] && python -m pylint $(git ls-files '*.py') to ensure no unresolved-import warnings.\n   • mypy --strict should report 0 \"Cannot find module\" errors.\n\n3. Runtime Smoke Test\n   • Start the application (docker-compose up or python main.py) and hit /health endpoint; expect HTTP 200.\n   • Trigger a code path that uses shared.simple_db (e.g. create/read/delete test record) to verify the module works post-import.\n\n4. Regression Gate\n   • Ensure the new pre-commit hook fails if a developer re-introduces an invalid import during a dummy commit.\n\nAll tests above must pass without manual intervention.",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Purge 50 Orphaned Modules With No Incoming References",
        "description": "Identify, validate, and physically remove 50 modules that are not referenced anywhere in the code-base to reduce bloat and build time.",
        "details": "1. Discovery\n   • Start from the latest dependency graph produced by our CI (build-dependency-graph step).\n   • Cross-check the list of 50 candidates supplied by Product/Architecture with a fresh run of `python -m pip install .[dev] && python -m depgraph --format=json > dep.json` to ensure they still have zero incoming edges.\n   • Run `pytest --cov` to collect runtime coverage; any candidate touched at runtime must be excluded or first migrated.\n\n2. Safety Validation\n   • Grep code-base for dynamic import usage: `grep -R \"__import__(\\|importlib\" src/ | grep <module_name>`.\n   • Scan docs and tooling scripts for hard-coded paths to each module.\n   • For any false-positive (e.g., used via entry-points), document and drop from the list.\n\n3. Staged Removal\n   • Branch: `feature/remove-orphaned-mods`.\n   • Delete module files and associated tests, fixtures, and data.\n   • Update `__all__` exports, `pyproject.toml`, `setup.cfg`, and `MANIFEST.in` to remove package declarations.\n   • Run `autoflake --remove-all-unused-imports -r src/` to clean now-unused imports in remaining code.\n\n4. Refactor & Clean-up\n   • Execute `pre-commit run -a` to apply formatting/linters.\n   • Run `scripts/build_size_check.sh` to capture size reduction metrics (goal: ≥ 8% src LOC reduction).\n\n5. Documentation & Changelog\n   • Add a section to `CHANGELOG.md` under “Breaking Changes – 2.x” noting removed modules.\n   • Update internal architecture diagram and wiki page \"Module Inventory\".\n\n6. PR Review Checklist\n   • All CI checks green.\n   • No new import-errors in `pylint` or `mypy` outputs.\n   • Sign-off from Tech Lead and Docs owner.",
        "testStrategy": "1. Static Analysis\n   • Re-generate dependency graph; assert all removed modules have zero nodes in the graph.\n   • Run `pylint` and `mypy` expecting no `E0401` or `error: Module not found`.\n\n2. Unit & Integration Tests\n   • `pytest -m \"not slow\"` locally, then full CI test matrix; all must pass.\n   • Ensure coverage % drop ≤ 1%; investigate if higher.\n\n3. Runtime Smoke Test\n   • Build and start the Docker image; run `python -m app.main --smoke` expecting exit code 0.\n\n4. Packaging Verification\n   • `pip install dist/*.whl` in a fresh venv; import top-level package without error.\n\n5. Build Size Assertion\n   • Compare `du -sh src/` before vs after; fail task if shrinkage < 5%.",
        "status": "pending",
        "dependencies": [
          1
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-08-19T04:07:20.362Z",
      "updated": "2025-08-19T04:08:18.686Z",
      "description": "Fix broken imports and clean up orphaned modules from dependency analysis"
    }
  }
}