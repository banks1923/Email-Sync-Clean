{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Fix and Validate Daily-Used vsearch CLI Commands",
        "description": "Identify and repair any broken or malfunctioning vsearch CLI commands that are actively used daily, ensuring they execute without errors and produce correct outputs.",
        "details": "1. Compile a list of all vsearch CLI commands currently used daily in the project environment.\n2. Execute each command in a controlled test environment to detect errors or incorrect outputs.\n3. For commands that fail or produce incorrect results, analyze error messages and logs to diagnose issues.\n4. Refer to the official vsearch manual and documentation to verify correct command syntax and expected behavior.\n5. Modify command usage, scripts, or underlying code to fix errors or incorrect outputs.\n6. Ensure compatibility with current vsearch version and dependencies.\n7. Document all changes made and update usage instructions accordingly.\n8. Coordinate with related tasks that involve vsearch command integration or testing to avoid conflicts.",
        "testStrategy": "1. Create a test suite that runs each daily-used vsearch command with representative input data.\n2. Verify that each command completes without error and produces expected output files or console output.\n3. Compare outputs against known correct results or baseline outputs.\n4. Include edge cases and typical usage scenarios in tests.\n5. Perform regression testing to confirm that fixes do not break other commands.\n6. Review logs for warnings or unexpected messages.\n7. Obtain user feedback from daily users to confirm practical functionality and performance.\n8. Automate tests where possible for continuous integration.",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Rename Confusing Field Names to Resolve Inconsistencies",
        "description": "Directly rename fields causing confusion and errors, focusing on 'relevance_score' vs 'semantic_score' and 'id' vs 'content_id' inconsistencies without adding compatibility layers.",
        "details": "1. Identify all instances in the codebase and database schema where 'relevance_score' and 'semantic_score' are used interchangeably or inconsistently. 2. Similarly, locate all uses of 'id' and 'content_id' fields that cause ambiguity or errors. 3. Decide on a consistent naming convention for these fields, preferably using descriptive, lowercase, underscore-separated names (e.g., 'relevance_score' and 'content_id') to align with best practices. 4. Rename the fields directly in the code, database schema, and any related configuration files or scripts. 5. Remove any legacy or deprecated references to old field names to prevent confusion. 6. Since this is for a single user with no compatibility layers needed, no backward compatibility code is required. 7. Document the changes clearly for future reference and maintenance.",
        "testStrategy": "1. Run unit and integration tests to verify that all references to the renamed fields function correctly without errors. 2. Test all workflows and features that depend on 'relevance_score', 'semantic_score', 'id', and 'content_id' to ensure data is correctly accessed and manipulated. 3. Validate database queries and updates involving these fields to confirm they execute successfully and return expected results. 4. Perform manual testing or code review to ensure no residual usage of old field names remains. 5. Confirm that no new errors or warnings related to field naming appear in logs or during runtime.",
        "status": "pending",
        "dependencies": [
          1
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Remove Dead Code and Clean Up Legacy Artifacts in Key Directories",
        "description": "Eliminate unused imports, deprecated functions, old compatibility shims, and any leftover code fragments cluttering the codebase within lib/, cli/, and infrastructure/mcp_servers/ directories.",
        "details": "1. Perform a thorough static analysis and manual review of the lib/, cli/, and infrastructure/mcp_servers/ directories to identify unused imports, deprecated functions, old compatibility shims, and any dead code remnants from previous refactorings.\n2. Use automated tools (e.g., linters, code coverage reports, and IDE features) to assist in detecting unused or unreachable code.\n3. Confirm that identified code is truly unused or obsolete by cross-referencing with current feature requirements and consulting with relevant developers if necessary.\n4. Remove all confirmed dead code, ensuring that no functionality is broken.\n5. Refactor affected files to maintain consistent formatting and naming conventions as per clean code best practices.\n6. Document the cleanup process and update any related documentation to reflect the removal of deprecated or obsolete code.\n7. Commit changes incrementally with clear commit messages describing the cleanup scope.",
        "testStrategy": "1. Run the full suite of unit, integration, and system tests to verify that no existing functionality is broken by the removal of dead code.\n2. Perform regression testing on features related to lib/, cli/, and infrastructure/mcp_servers/ to ensure stability.\n3. Use code coverage tools before and after cleanup to confirm that removed code was indeed unused.\n4. Conduct peer code reviews focusing on the correctness and completeness of the cleanup.\n5. Monitor the application in staging environments for any runtime errors or warnings related to the cleaned-up areas.\n6. Validate that build and deployment pipelines remain unaffected by the code removal.",
        "status": "pending",
        "dependencies": [
          1
        ],
        "priority": "low",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Set Up Stoneman Case Search Database Using Python Scripts",
        "description": "Implement the Stoneman case search database by executing and integrating the provided Python scripts: stoneman_search.py, stoneman_litigation.py, and stoneman_exhibits.py.",
        "details": "1. Review the Python scripts (stoneman_search.py, stoneman_litigation.py, stoneman_exhibits.py) to understand their roles in data ingestion, indexing, and search functionality.\n2. Establish database connections using appropriate Python database libraries (e.g., sqlite3 or MySQL connectors) ensuring secure and efficient access.\n3. Execute the scripts to create and populate the search database with case data, litigation details, and exhibits.\n4. Implement parameterized SQL queries within the scripts to prevent SQL injection and ensure robust querying.\n5. Integrate the scripts to work cohesively, enabling comprehensive search capabilities across cases, litigation, and exhibits.\n6. Handle exceptions and errors gracefully, including database locking, connection issues, and data inconsistencies.\n7. Optimize database schema and indexing to support efficient search queries.\n8. Document the setup process and any configuration required for deployment or future maintenance.",
        "testStrategy": "1. Verify that the database is correctly created and populated by each script without errors.\n2. Run sample search queries through the integrated system to confirm accurate and relevant results across cases, litigation, and exhibits.\n3. Test edge cases such as empty search terms, special characters, and large data volumes.\n4. Confirm that parameterized queries prevent SQL injection vulnerabilities.\n5. Monitor for and resolve any database locking or concurrency issues during script execution.\n6. Perform regression testing to ensure no disruption to existing functionalities.\n7. Review logs and error handling to ensure all exceptions are properly managed and reported.",
        "status": "pending",
        "dependencies": [
          1,
          2,
          3
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Generate Discovery Questions and Deposition Contradictions for Stoneman Case Using stoneman_litigation.py",
        "description": "Develop functionality to automatically generate discovery questions and identify deposition contradictions for the Stoneman case by leveraging the stoneman_litigation.py script.",
        "details": "1. Analyze the stoneman_litigation.py script to understand its data structures, functions, and how it processes litigation information relevant to the Stoneman case.\n2. Design algorithms to extract or formulate discovery questions covering key litigation topics such as financial details, property, custody, and other relevant areas, based on legal discovery standards and examples.\n3. Implement logic to detect contradictions within deposition transcripts or related documents by comparing statements for inconsistencies.\n4. Integrate these features into the existing system, ensuring compatibility with the Stoneman case search database established in Task 4.\n5. Provide clear, categorized output of generated discovery questions and flagged deposition contradictions for review.\n6. Document the code and usage instructions for maintainability and future enhancements.",
        "testStrategy": "1. Validate that the generated discovery questions align with recognized legal discovery categories and include relevant, case-specific content.\n2. Test the contradiction detection by inputting sample deposition data with known inconsistencies and verifying accurate identification.\n3. Perform integration testing with the Stoneman case database to ensure seamless data flow and correct referencing.\n4. Conduct edge case testing with incomplete or ambiguous deposition data to assess robustness.\n5. Review output for clarity, completeness, and correctness by legal experts or domain specialists if possible.",
        "status": "pending",
        "dependencies": [
          4
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Create Litigation Exhibits Showing Repair Avoidance Pattern and Timeline of Violations Using stoneman_exhibits.py",
        "description": "Develop functionality to generate litigation exhibits that illustrate the repair avoidance pattern with an 87.8% excuse rate and a detailed timeline of violations for the Stoneman case using the stoneman_exhibits.py script.",
        "details": "1. Analyze the stoneman_exhibits.py script to understand its current capabilities, data inputs, and output formats related to exhibit generation.\n2. Design data extraction methods to identify and quantify repair avoidance patterns, specifically calculating and highlighting the 87.8% excuse rate from case data.\n3. Develop a timeline visualization feature that chronologically maps violations relevant to the Stoneman case, ensuring clarity and legal relevance.\n4. Integrate these features into the script to produce exhibits suitable for litigation presentation, ensuring compatibility with existing exhibit management workflows.\n5. Ensure exhibits are exportable in common legal presentation formats (e.g., PDF, image files) and support annotations if needed.\n6. Document the new functionality within the codebase and provide usage instructions for legal teams.",
        "testStrategy": "1. Validate that the exhibits accurately reflect the repair avoidance pattern by cross-checking the calculated excuse rate against source data.\n2. Verify the timeline of violations is complete, correctly ordered, and visually clear.\n3. Test exhibit generation with various data subsets to ensure robustness and correctness.\n4. Confirm that the output exhibits are compatible with standard litigation presentation tools and formats.\n5. Conduct user acceptance testing with legal professionals to ensure the exhibits meet litigation needs and are easy to interpret.\n6. Perform regression testing on stoneman_exhibits.py to ensure existing functionality remains unaffected.",
        "status": "pending",
        "dependencies": [
          4,
          5
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Isolated Cloud Embedding Service with Environment-Based Provider Switching and A/B Testing",
        "description": "Develop a cloud embedding service using a factory pattern to isolate local, cloud, and mock embedding providers, enabling environment-based switching and supporting A/B comparison tests between local and cloud embeddings.",
        "details": "1. Design and implement an embedding factory pattern that abstracts the creation and management of embedding providers, ensuring clear separation of concerns between local, cloud, and mock implementations.\n2. Develop isolated cloud embedding provider modules that can be tested independently from other system components.\n3. Implement environment-based configuration to dynamically switch between embedding providers (local, cloud, mock) based on deployment or runtime settings.\n4. Create comprehensive unit and integration tests to verify the correct instantiation and operation of each embedding provider in isolation.\n5. Develop A/B testing infrastructure to compare the performance, accuracy, and resource usage of local versus cloud embeddings under controlled conditions.\n6. Integrate logging and monitoring to capture metrics relevant to embedding quality and service reliability.\n7. Ensure the design supports extensibility for future embedding providers and maintains clean interfaces to facilitate maintenance and testing.",
        "testStrategy": "1. Unit test the embedding factory to confirm it returns the correct provider instance based on environment configuration.\n2. Isolate and test each embedding provider (local, cloud, mock) to verify correct embedding generation and error handling.\n3. Conduct environment-based integration tests to ensure seamless switching between providers without side effects.\n4. Perform A/B comparison tests by running identical input data through local and cloud embeddings, measuring and comparing output quality metrics and performance.\n5. Validate logging and monitoring capture expected metrics and alerts.\n6. Conduct regression tests to confirm existing embedding-related functionality remains unaffected.\n7. Use mock providers to simulate failure scenarios and verify system resilience and fallback behavior.",
        "status": "pending",
        "dependencies": [
          3
        ],
        "priority": "high",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-09-05T06:00:33.150Z",
      "updated": "2025-09-05T09:24:48.847Z",
      "description": "Tasks for master context"
    }
  }
}