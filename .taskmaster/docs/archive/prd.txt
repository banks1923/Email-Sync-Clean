# Product Requirements Document: Direct Service Alignment - Zero Adapters

## Project Overview
**Goal:** Remove all adapter patterns and compatibility shims by directly aligning service interfaces, implementing proper schema migrations, and adding robust preflight checks.

**Scope:** Fix root causes of API mismatches instead of hiding them behind adapters.

**Timeline:** 100 minutes estimated implementation time

## Problem Statement
Three categories of service misalignment currently exist:
1. **Parameter mismatches**: `save_to_db` passed but not accepted by EmailThreadProcessor
2. **Missing API methods**: `get_all_content_ids` expected but not provided by SimpleDB
3. **Schema drift**: Missing database columns (`source_path`, `vector_processed`, `word_count`)

Current adapters (created earlier) add complexity without solving root issues.

## Solution Architecture

### Core Principles
- **Direct alignment** - Fix mismatches at the source
- **Proper migrations** - SQL files, no runtime ALTERs
- **Fail fast** - Comprehensive preflight checks
- **Clean interfaces** - Exact method signatures, no kwargs

## Implementation Phases

### Phase 1: Add Missing SimpleDB Methods (30 min)

#### Task 1.1: Implement Core SimpleDB Methods
Add these exact methods to `shared/simple_db.py`:

```python
def get_all_content_ids(self, content_type: str = None) -> list[str]:
    """Get all content IDs, optionally filtered by type."""
    if content_type:
        query = "SELECT id FROM content WHERE content_type = ? ORDER BY created_time DESC"
        rows = self.fetch(query, (content_type,))
    else:
        query = "SELECT id FROM content ORDER BY created_time DESC"
        rows = self.fetch(query)
    return [row["id"] for row in rows]

def get_content_by_ids(self, ids: list[str]) -> list[dict]:
    """Batch fetch content by IDs with chunking for SQLite limits."""
    if not ids:
        return []
    results = []
    for chunk in [ids[i:i+500] for i in range(0, len(ids), 500)]:
        placeholders = ",".join("?" * len(chunk))
        query = f"SELECT * FROM content WHERE id IN ({placeholders})"
        results.extend(self.fetch(query, tuple(chunk)))
    return results

def mark_content_vectorized(self, content_id: str) -> bool:
    """Mark single content as vectorized."""
    cursor = self.execute(
        "UPDATE content SET vector_processed = 1 WHERE id = ?",
        (content_id,)
    )
    return cursor.rowcount > 0

def batch_mark_vectorized(self, content_ids: list[str]) -> int:
    """Mark multiple contents as vectorized with SQLite limit handling."""
    if not content_ids:
        return 0
    total_updated = 0
    for chunk in [content_ids[i:i+500] for i in range(0, len(content_ids), 500)]:
        placeholders = ",".join("?" * len(chunk))
        query = f"UPDATE content SET vector_processed = 1 WHERE id IN ({placeholders})"
        cursor = self.execute(query, tuple(chunk))
        total_updated += cursor.rowcount
    return total_updated
```

#### Task 1.2: Add Performance Indexes
Add to SimpleDB initialization or migration:

```sql
CREATE INDEX IF NOT EXISTS idx_content_type ON content(content_type);
CREATE INDEX IF NOT EXISTS idx_vector_processed ON content(vector_processed);
CREATE INDEX IF NOT EXISTS idx_content_hash ON content(content_hash);
```

### Phase 2: Create Schema Migration System (20 min)

#### Task 2.1: Create Migration Directory Structure
```
migrations/
├── 001_initial_schema.sql
├── 002_add_vector_columns.sql
├── applied_migrations.txt
└── migrate.py
```

#### Task 2.2: Create Migration SQL File
File: `migrations/002_add_vector_columns.sql`
```sql
-- Add missing columns for vector processing
ALTER TABLE content ADD COLUMN source_path TEXT;
ALTER TABLE content ADD COLUMN vector_processed INTEGER DEFAULT 0;
ALTER TABLE content ADD COLUMN word_count INTEGER DEFAULT 0;

-- Add performance indexes
CREATE INDEX IF NOT EXISTS idx_content_type ON content(content_type);
CREATE INDEX IF NOT EXISTS idx_vector_processed ON content(vector_processed);
CREATE INDEX IF NOT EXISTS idx_content_hash ON content(content_hash);
```

#### Task 2.3: Create Migration Runner
File: `migrations/migrate.py` (30 lines max)
```python
#!/usr/bin/env python3
"""Simple migration runner - no runtime ALTERs in app code."""

import sqlite3
from pathlib import Path
from loguru import logger

def run_migrations(db_path: str = "data/emails.db"):
    """Apply pending SQL migrations."""
    migrations_dir = Path(__file__).parent
    applied_file = migrations_dir / "applied_migrations.txt"
    
    # Load applied migrations
    applied = set()
    if applied_file.exists():
        applied = set(applied_file.read_text().strip().split('\n'))
    
    # Find pending migrations
    sql_files = sorted(migrations_dir.glob("*.sql"))
    pending = [f for f in sql_files if f.name not in applied]
    
    if not pending:
        logger.info("No pending migrations")
        return
    
    # Apply each migration
    conn = sqlite3.connect(db_path)
    for migration in pending:
        logger.info(f"Applying {migration.name}")
        conn.executescript(migration.read_text())
        applied.add(migration.name)
    
    # Save applied migrations
    applied_file.write_text('\n'.join(sorted(applied)))
    logger.info(f"Applied {len(pending)} migrations")

if __name__ == "__main__":
    run_migrations()
```

### Phase 3: Remove All Shims (20 min)

#### Task 3.1: Remove save_to_db Parameter
Files to modify:
- `gmail/main.py:416` - Remove `save_to_db=True`
- `tests/infrastructure/test_email_thread_processor.py:342,379` - Remove parameter

#### Task 3.2: Delete Adapter Directory
- Delete entire `adapters/` directory

#### Task 3.3: Remove Adapter Imports
- `gmail/main.py:411` - Remove adapter import and usage
- `utilities/maintenance/vector_maintenance.py:38` - Remove adapter import

#### Task 3.4: Remove Compatibility Helpers
- `utilities/maintenance/vector_maintenance.py:44-72` - Delete `_db_get_all_content_ids` helper

### Phase 4: Implement Preflight Checks (15 min)

#### Task 4.1: Create Preflight Check System
File: `utilities/maintenance/preflight.py`
```python
"""Comprehensive preflight checks - fail fast with clear errors."""

import numpy as np
from loguru import logger
from shared.simple_db import SimpleDB
from utilities.embeddings import get_embedding_service
from utilities.vector_store import get_vector_store

def preflight_check():
    """Verify system readiness before any operations."""
    errors = []
    
    # 1. Check Qdrant running
    try:
        vector_store = get_vector_store()
        collections = vector_store.client.get_collections()
    except:
        errors.append("Qdrant not running. Start: QDRANT__STORAGE__PATH=./qdrant_data ~/bin/qdrant &")
    
    # 2. Verify embedding service
    try:
        emb_service = get_embedding_service()
        test_vec = emb_service.encode("test")
        
        if len(test_vec) != 1024:
            errors.append(f"Wrong embedding dim: {len(test_vec)} (expected 1024)")
        
        l2_norm = np.linalg.norm(test_vec)
        if abs(l2_norm - 1.0) > 0.02:
            errors.append(f"Embeddings not L2-normalized: {l2_norm:.3f}")
    except Exception as e:
        errors.append(f"Embedding service failed: {e}")
    
    # 3. Verify collection dimensions
    try:
        expected_dim = 1024
        for collection in ["emails", "pdfs", "transcriptions"]:
            stats = vector_store.get_collection_stats(collection)
            if stats and stats.get("vector_size"):
                if stats["vector_size"] != expected_dim:
                    errors.append(f"{collection} dim mismatch: {stats['vector_size']} != {expected_dim}")
    except:
        pass  # Collections may not exist yet
    
    # 4. Check database schema
    db = SimpleDB()
    cursor = db.execute("PRAGMA table_info(content)")
    existing_cols = {col[1] for col in cursor.fetchall()}
    required_cols = {"id", "content_type", "title", "content", "source_path", 
                    "vector_processed", "word_count", "content_hash"}
    missing = required_cols - existing_cols
    
    if missing:
        errors.append(f"Missing columns: {missing}. Run: python migrations/migrate.py")
    
    # 5. Verify required methods exist
    required_methods = ["get_all_content_ids", "get_content_by_ids", 
                       "mark_content_vectorized", "batch_mark_vectorized"]
    for method in required_methods:
        if not hasattr(db, method):
            errors.append(f"SimpleDB missing method: {method}")
    
    # Report results
    if errors:
        for err in errors:
            logger.error(f"❌ {err}")
        raise RuntimeError(f"{len(errors)} preflight checks failed")
    
    logger.info("✅ All preflight checks passed")
    return True
```

### Phase 5: Update Service Interfaces (15 min)

#### Task 5.1: Standardize Embedding Service
Update all calls from `embed_text()` to `encode()`:
- Search and replace across codebase
- Ensure consistent API: `encode(text: str) -> np.ndarray`

#### Task 5.2: Standardize Vector Store
Ensure these exact methods:
- `batch_upsert(collection: str, points: list) -> dict`
- `iter_ids(collection: str, page_size: int = 100) -> Iterator[str]`
- `delete_many(collection: str, ids: list[str]) -> dict`
- `get_collection_stats(collection: str) -> dict`

### Phase 6: Testing and Verification (15 min)

#### Task 6.1: Create Contract Tests
File: `tests/test_contracts.py`
```python
"""Verify service contracts are met."""

import numpy as np
import pytest
from shared.simple_db import SimpleDB
from utilities.embeddings import get_embedding_service
from utilities.vector_store import get_vector_store

def test_embedding_contract():
    """Verify embedding service meets requirements."""
    service = get_embedding_service()
    
    # Test single encoding
    vec = service.encode("test")
    assert len(vec) == 1024, f"Wrong dim: {len(vec)}"
    assert vec.dtype == np.float32, f"Wrong dtype: {vec.dtype}"
    assert abs(np.linalg.norm(vec) - 1.0) < 0.02, "Not L2-normalized"
    
    # Test empty string
    empty_vec = service.encode("")
    assert len(empty_vec) == 1024
    assert np.allclose(empty_vec, 0), "Empty should return zeros"

def test_simpledb_contract():
    """Verify SimpleDB meets requirements."""
    db = SimpleDB()
    
    # Test required methods exist
    assert hasattr(db, "get_all_content_ids")
    assert hasattr(db, "get_content_by_ids")
    assert hasattr(db, "mark_content_vectorized")
    assert hasattr(db, "batch_mark_vectorized")
    
    # Test batch_mark_vectorized handles large batches
    large_ids = [f"test_{i}" for i in range(1500)]
    count = db.batch_mark_vectorized(large_ids)
    # Should not raise SQLite variable limit error

def test_vector_store_contract():
    """Verify vector store meets requirements."""
    store = get_vector_store()
    
    # Test required methods exist
    assert hasattr(store, "batch_upsert")
    assert hasattr(store, "iter_ids")
    assert hasattr(store, "get_collection_stats")
```

#### Task 6.2: Run Integration Tests
```bash
# Run migrations first
python migrations/migrate.py

# Run preflight checks
python -c "from utilities.maintenance.preflight import preflight_check; preflight_check()"

# Run contract tests
pytest tests/test_contracts.py -v

# Quick smoke test
make vector-smoke

# Full pipeline test
make full-run
```

## Success Criteria

1. **No adapters remain** - `adapters/` directory deleted
2. **All preflight checks pass** - Clean startup
3. **Contract tests pass** - Services meet interface requirements
4. **Integration tests pass** - `make full-run` succeeds
5. **Performance maintained** - p95 search latency <100ms
6. **Clean logs** - No warnings about missing methods

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| SQLite variable limit | Chunk all batch operations to 500 items |
| Embedding dimension mismatch | Preflight validates before processing |
| Missing columns break queries | Migration system ensures correct schema |
| Performance regression | Add indexes, monitor p95 latencies |

## Files to Modify

**Core Changes (8 files):**
1. `shared/simple_db.py` - Add 4 methods (~60 lines)
2. `migrations/002_add_vector_columns.sql` - New migration (~10 lines)
3. `migrations/migrate.py` - Migration runner (~30 lines)
4. `utilities/maintenance/preflight.py` - Preflight checks (~70 lines)
5. `gmail/main.py` - Remove adapter and save_to_db (~5 lines removed)
6. `utilities/maintenance/vector_maintenance.py` - Remove adapter (~35 lines removed)
7. `tests/infrastructure/test_email_thread_processor.py` - Remove save_to_db (~2 lines)
8. `tests/test_contracts.py` - New contract tests (~50 lines)

**Deletions:**
- `adapters/` directory - Complete removal (~400 lines)

**Net Impact:** +220 lines added, -440 lines removed = **220 lines reduction**

## Validation Commands

```bash
# Verify no adapters remain
ls -la adapters/ 2>/dev/null || echo "✅ Adapters removed"

# Check for adapter imports
grep -r "from adapters import" --include="*.py" || echo "✅ No adapter imports"

# Verify schema
sqlite3 data/emails.db "PRAGMA table_info(content)" | grep -E "source_path|vector_processed|word_count"

# Run tests
make check
make full-run
```

## Post-Implementation Checklist

- [ ] All SimpleDB methods implemented with chunking
- [ ] Migration system created and run
- [ ] All adapters removed
- [ ] save_to_db parameter removed everywhere
- [ ] Preflight checks pass
- [ ] Contract tests pass
- [ ] Integration tests pass
- [ ] Performance verified (<100ms p95)
- [ ] Documentation updated

---

**This PRD is ready for Task Master parsing.** Each task is atomic, concrete, and testable. The phases can be executed sequentially with clear validation at each step.