# Email Sync System - Duplicate Prevention & Data Cleanup PRD

## Executive Summary
The Email Sync system currently has 55 duplicate emails and 45 test data entries polluting the database. Additionally, there's no database-level protection against future duplicates in the content table. This PRD outlines a comprehensive solution to clean existing data and prevent future duplicates through multi-layer protection.

## Problem Statement
1. **Current Issues**:
   - 536 emails in database but only 481 are unique (55 duplicates)
   - 45 test data entries contaminating production data
   - No UNIQUE constraints on content table allowing duplicates
   - Orphaned vectors in Qdrant (546 vectors vs 547 content items)
   - 17 emails with empty titles indicating broken syncs

2. **Impact**:
   - Search results show duplicate entries
   - Vector storage wasted on duplicates
   - Test data appearing in production searches
   - Database integrity compromised
   - User confusion from duplicate results

## Goals & Success Criteria

### Primary Goals
1. Remove all test data and duplicates from the database
2. Implement database-level duplicate prevention
3. Add application-level deduplication logic
4. Create monitoring for duplicate detection
5. Ensure data integrity across all tables

### Success Metrics
- Zero duplicate content_hash values in content table
- Only emails from 10 configured senders remain
- Vector count exactly matches content count
- No test data patterns in any table
- Future duplicate attempts automatically rejected

## Current State Analysis

### Database Statistics
- **Content Table**: 547 total entries
  - Emails: 536 (481 unique)
  - PDFs: 11 (all unique)
- **Legitimate Data**: 492 emails from configured senders
- **Test Data**: 45 entries to remove
- **Duplicates**: 55 email duplicates
- **Vectors**: 546 in Qdrant (1 orphan)

### Test Data Identified
1. **Contract Review Test Data** (27 entries):
   - Title: "Contract Review - ABC Corp Agreement"
   - Created: 2025-08-16 between 01:47-01:51
   - Synthetic test data with multiple duplicates

2. **Test Subject Emails** (15 entries):
   - Titles: "Test Subject 0", "Test Subject 1", "Test Subject 2"
   - 5 duplicates of each
   - Created: 2025-08-16 01:47

3. **Miscellaneous Test Emails** (3 entries):
   - "Test Email with Rich Display"
   - "Test Email Subject for Export" (from test@example.com)
   - "RE: Testing"

4. **Empty/Broken Emails** (17 entries):
   - Emails with empty titles and minimal content
   - Likely incomplete syncs or data corruption

### Legitimate Duplicates Found
- "Re: 518 N Stoneman Ave Move In Form" (2 copies)
- "Zelle® payment of $3,250.00 to Jennifer Burbank" (2 copies)
- Several other emails with 2-3 duplicates each

## Technical Requirements

### Database Schema Changes

#### 1. Content Table Modifications
- Add `content_hash` column (TEXT, NOT NULL)
- Create UNIQUE INDEX on content_hash
- Backfill hashes for existing content
- Handle collisions by keeping earliest entry

#### 2. Hash Calculation Method
```python
def calculate_content_hash(content_type, title, content):
    """Generate SHA-256 hash for deduplication"""
    normalized_title = (title or "").strip().lower()
    normalized_content = (content or "").strip()
    hash_input = f"{content_type}:{normalized_title}:{normalized_content}"
    return hashlib.sha256(hash_input.encode('utf-8')).hexdigest()
```

### Application Layer Changes

#### 1. SimpleDB.add_content() Updates
- Calculate content_hash before insert
- Use INSERT OR IGNORE pattern
- Check for existing hash and return existing content_id
- Log duplicate attempts for monitoring

#### 2. PDF Service Deduplication
- Check content_hash before expensive OCR processing
- Use both file_hash and content_hash for dual validation
- Skip processing if content already exists
- Return existing content_id for duplicates

#### 3. Gmail Service Enhancement
- Continue using existing content_hash in emails table
- Ensure content table entries also have hash
- Coordinate between emails and content tables

### Data Cleanup Operations

#### 1. Test Data Removal
- Delete all "Contract Review - ABC Corp Agreement" entries
- Remove all "Test Subject" emails (0, 1, 2)
- Clean miscellaneous test emails
- Remove emails with empty titles and minimal content
- Clean corresponding vectors from Qdrant

#### 2. Duplicate Resolution
- Identify duplicates by content_hash
- Keep earliest created_time entry
- Update any foreign key references
- Remove duplicate entries
- Clean orphaned vectors

### Monitoring & Prevention

#### 1. Duplicate Detection Script
- Daily scheduled check for duplicates
- Compare content count vs vector count
- Detect test data patterns
- Alert on anomalies
- Generate integrity reports

#### 2. Pre-Processing Validation
- Check hash before expensive operations (OCR, embedding)
- Cache recent hashes in memory
- Quick duplicate detection
- Skip redundant processing

#### 3. Vector Store Sync
- Use content_id as vector ID
- Check existence before insert
- Update rather than duplicate
- Regular sync validation

## Implementation Phases

### Phase 1: Database Protection (Critical)
1. Add content_hash column to content table
2. Create UNIQUE INDEX on content_hash
3. Write migration script for existing data
4. Test constraint enforcement

### Phase 2: Application Updates (Important)
1. Update SimpleDB.add_content() with hash calculation
2. Modify PDF service for deduplication
3. Enhance Gmail sync coordination
4. Add pre-processing validation

### Phase 3: Data Cleanup (Required)
1. Remove 45 test data entries
2. Clean 55 duplicate emails
3. Remove 17 empty/broken emails
4. Sync vectors with cleaned content

### Phase 4: Monitoring (Ongoing)
1. Create duplicate detection script
2. Set up daily monitoring job
3. Implement alerting system
4. Generate regular integrity reports

## Risk Mitigation

### Potential Risks
1. **Data Loss**: Accidentally deleting legitimate data
   - Mitigation: Backup database before cleanup
   - Validate sender email addresses against config

2. **Hash Collisions**: Different content generating same hash
   - Mitigation: Use SHA-256 (extremely low collision probability)
   - Include content_type in hash calculation

3. **Performance Impact**: Hash calculation overhead
   - Mitigation: Index on content_hash for fast lookups
   - Cache recent hashes in memory

4. **Foreign Key Issues**: Orphaned references after cleanup
   - Mitigation: Check and update all foreign keys
   - Validate referential integrity post-cleanup

## Testing Requirements

### Unit Tests
- Hash calculation consistency
- Duplicate detection logic
- INSERT OR IGNORE behavior
- Foreign key updates

### Integration Tests
- End-to-end duplicate prevention
- Gmail sync with deduplication
- PDF upload with existing content
- Vector store synchronization

### Validation Tests
- No test data remains
- All duplicates removed
- Vector count matches content
- Only legitimate emails present

## Rollback Plan

If issues occur:
1. Restore from database backup
2. Remove content_hash constraint temporarily
3. Revert application code changes
4. Investigate and fix issues
5. Retry implementation

## Documentation Updates

Required documentation:
1. Update CLAUDE.md with deduplication strategy
2. Document hash calculation method
3. Add monitoring script usage
4. Update troubleshooting guide

## Acceptance Criteria

The implementation is complete when:
1. ✅ Content table has content_hash column with UNIQUE constraint
2. ✅ All test data removed (45 entries)
3. ✅ All duplicates removed (55 entries)
4. ✅ Vector count equals content count
5. ✅ SimpleDB.add_content() uses INSERT OR IGNORE
6. ✅ PDF service checks for duplicates before processing
7. ✅ Monitoring script created and tested
8. ✅ No test patterns in database
9. ✅ Only emails from configured senders remain
10. ✅ Documentation updated

## Timeline Estimate

- Phase 1 (Database Protection): 2-3 hours
- Phase 2 (Application Updates): 3-4 hours
- Phase 3 (Data Cleanup): 2-3 hours
- Phase 4 (Monitoring): 2 hours
- Testing & Validation: 2 hours

Total: 11-14 hours of implementation

## Appendix: SQL Queries

### Find Duplicates
```sql
SELECT title, COUNT(*) as count 
FROM content 
GROUP BY title, content 
HAVING COUNT(*) > 1 
ORDER BY count DESC;
```

### Remove Test Data
```sql
DELETE FROM content WHERE title = 'Contract Review - ABC Corp Agreement';
DELETE FROM content WHERE title IN ('Test Subject 0', 'Test Subject 1', 'Test Subject 2');
DELETE FROM content WHERE title LIKE '%Test Email%' OR title = 'RE: Testing';
DELETE FROM content WHERE metadata LIKE '%test@example.com%';
DELETE FROM content WHERE (title = '' OR title IS NULL) AND LENGTH(content) < 100;
```

### Add Hash Column
```sql
ALTER TABLE content ADD COLUMN content_hash TEXT;
CREATE UNIQUE INDEX idx_content_hash ON content(content_hash);
```

## End of PRD